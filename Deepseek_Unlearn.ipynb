{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!wget https://bootstrap.pypa.io/get-pip.py\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ogXd7_tN3TTX",
        "outputId": "47d1480f-4ff1-46d4-c66e-28f96ccfe38d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-05-20 04:06:23--  https://bootstrap.pypa.io/get-pip.py\n",
            "Resolving bootstrap.pypa.io (bootstrap.pypa.io)... 151.101.0.175, 151.101.64.175, 151.101.128.175, ...\n",
            "Connecting to bootstrap.pypa.io (bootstrap.pypa.io)|151.101.0.175|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2279307 (2.2M) [text/x-python]\n",
            "Saving to: ‘get-pip.py’\n",
            "\n",
            "\rget-pip.py            0%[                    ]       0  --.-KB/s               \rget-pip.py          100%[===================>]   2.17M  --.-KB/s    in 0.07s   \n",
            "\n",
            "2025-05-20 04:06:23 (29.4 MB/s) - ‘get-pip.py’ saved [2279307/2279307]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3.10 get-pip.py\n"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZrRlTF4s3yk1",
        "outputId": "20a81183-7544-44d7-89c1-7a3fff18d578"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pip\n",
            "  Downloading pip-25.1.1-py3-none-any.whl.metadata (3.6 kB)\n",
            "Collecting setuptools\n",
            "  Downloading setuptools-80.7.1-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting wheel\n",
            "  Downloading wheel-0.45.1-py3-none-any.whl.metadata (2.3 kB)\n",
            "Downloading pip-25.1.1-py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m37.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading setuptools-80.7.1-py3-none-any.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m66.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wheel-0.45.1-py3-none-any.whl (72 kB)\n",
            "Installing collected packages: wheel, setuptools, pip\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/3\u001b[0m [pip]\n",
            "\u001b[1A\u001b[2KSuccessfully installed pip-25.1.1 setuptools-80.7.1 wheel-0.45.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python --version\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "keCRW_HjfP6r",
        "outputId": "9b91866f-16b1-42f1-bd18-3ef8351928a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.11.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "bckhH3CpgcAC",
        "outputId": "de0f7a12-c70b-4a30-d15c-b291a575937c"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-6ad5a27b-ef59-4038-a3eb-77282ec365be\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-6ad5a27b-ef59-4038-a3eb-77282ec365be\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving UGBench.zip to UGBench.zip\n"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()  # Choose your .zip or .py files\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip UGBench.zip -d Unlearn_Harry"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "XfJJzeO1iqLf",
        "outputId": "5cd5cdce-5bd9-4672-c0c4-1924fd7ed9e7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  UGBench.zip\n",
            "   creating: Unlearn_Harry/UGBench/\n",
            "   creating: Unlearn_Harry/UGBench/.git/\n",
            "  inflating: Unlearn_Harry/UGBench/.git/config  \n",
            "  inflating: Unlearn_Harry/UGBench/.git/description  \n",
            "  inflating: Unlearn_Harry/UGBench/.git/HEAD  \n",
            "   creating: Unlearn_Harry/UGBench/.git/hooks/\n",
            "  inflating: Unlearn_Harry/UGBench/.git/hooks/applypatch-msg.sample  \n",
            "  inflating: Unlearn_Harry/UGBench/.git/hooks/commit-msg.sample  \n",
            "  inflating: Unlearn_Harry/UGBench/.git/hooks/fsmonitor-watchman.sample  \n",
            "  inflating: Unlearn_Harry/UGBench/.git/hooks/post-update.sample  \n",
            "  inflating: Unlearn_Harry/UGBench/.git/hooks/pre-applypatch.sample  \n",
            "  inflating: Unlearn_Harry/UGBench/.git/hooks/pre-commit.sample  \n",
            "  inflating: Unlearn_Harry/UGBench/.git/hooks/pre-merge-commit.sample  \n",
            "  inflating: Unlearn_Harry/UGBench/.git/hooks/pre-push.sample  \n",
            "  inflating: Unlearn_Harry/UGBench/.git/hooks/pre-rebase.sample  \n",
            "  inflating: Unlearn_Harry/UGBench/.git/hooks/pre-receive.sample  \n",
            "  inflating: Unlearn_Harry/UGBench/.git/hooks/prepare-commit-msg.sample  \n",
            "  inflating: Unlearn_Harry/UGBench/.git/hooks/push-to-checkout.sample  \n",
            "  inflating: Unlearn_Harry/UGBench/.git/hooks/sendemail-validate.sample  \n",
            "  inflating: Unlearn_Harry/UGBench/.git/hooks/update.sample  \n",
            "  inflating: Unlearn_Harry/UGBench/.git/index  \n",
            "   creating: Unlearn_Harry/UGBench/.git/info/\n",
            "  inflating: Unlearn_Harry/UGBench/.git/info/exclude  \n",
            "   creating: Unlearn_Harry/UGBench/.git/logs/\n",
            "  inflating: Unlearn_Harry/UGBench/.git/logs/HEAD  \n",
            "   creating: Unlearn_Harry/UGBench/.git/logs/refs/\n",
            "   creating: Unlearn_Harry/UGBench/.git/logs/refs/heads/\n",
            "  inflating: Unlearn_Harry/UGBench/.git/logs/refs/heads/main  \n",
            "   creating: Unlearn_Harry/UGBench/.git/logs/refs/remotes/\n",
            "   creating: Unlearn_Harry/UGBench/.git/logs/refs/remotes/origin/\n",
            "  inflating: Unlearn_Harry/UGBench/.git/logs/refs/remotes/origin/HEAD  \n",
            "   creating: Unlearn_Harry/UGBench/.git/objects/\n",
            "   creating: Unlearn_Harry/UGBench/.git/objects/info/\n",
            "   creating: Unlearn_Harry/UGBench/.git/objects/pack/\n",
            "  inflating: Unlearn_Harry/UGBench/.git/objects/pack/pack-992a1282df50ffbf2d3546b6c345f3d2f0e0a9aa.idx  \n",
            "  inflating: Unlearn_Harry/UGBench/.git/objects/pack/pack-992a1282df50ffbf2d3546b6c345f3d2f0e0a9aa.pack  \n",
            "  inflating: Unlearn_Harry/UGBench/.git/objects/pack/pack-992a1282df50ffbf2d3546b6c345f3d2f0e0a9aa.rev  \n",
            "  inflating: Unlearn_Harry/UGBench/.git/packed-refs  \n",
            "   creating: Unlearn_Harry/UGBench/.git/refs/\n",
            "   creating: Unlearn_Harry/UGBench/.git/refs/heads/\n",
            "  inflating: Unlearn_Harry/UGBench/.git/refs/heads/main  \n",
            "   creating: Unlearn_Harry/UGBench/.git/refs/remotes/\n",
            "   creating: Unlearn_Harry/UGBench/.git/refs/remotes/origin/\n",
            "  inflating: Unlearn_Harry/UGBench/.git/refs/remotes/origin/HEAD  \n",
            "   creating: Unlearn_Harry/UGBench/.git/refs/tags/\n",
            "  inflating: Unlearn_Harry/UGBench/.gitignore  \n",
            "  inflating: Unlearn_Harry/UGBench/aggregate_eval_stat.py  \n",
            "   creating: Unlearn_Harry/UGBench/config/\n",
            "  inflating: Unlearn_Harry/UGBench/config/aggregate_eval_stat.yaml  \n",
            "  inflating: Unlearn_Harry/UGBench/config/ds_config.json  \n",
            "  inflating: Unlearn_Harry/UGBench/config/eval_harry.yaml  \n",
            "  inflating: Unlearn_Harry/UGBench/config/eval_tofu.yaml  \n",
            "  inflating: Unlearn_Harry/UGBench/config/eval_zsre_inverse.yaml  \n",
            "  inflating: Unlearn_Harry/UGBench/config/eval_zsre_onehop.yaml  \n",
            "  inflating: Unlearn_Harry/UGBench/config/eval_zsre_subject_replace.yaml  \n",
            "  inflating: Unlearn_Harry/UGBench/config/finetune.yaml  \n",
            "  inflating: Unlearn_Harry/UGBench/config/forget.yaml  \n",
            "  inflating: Unlearn_Harry/UGBench/config/model_config.yaml  \n",
            "   creating: Unlearn_Harry/UGBench/data/\n",
            "   creating: Unlearn_Harry/UGBench/data/Harry/\n",
            "  inflating: Unlearn_Harry/UGBench/data/Harry/finetune.json  \n",
            "  inflating: Unlearn_Harry/UGBench/data/Harry/forget.json  \n",
            "  inflating: Unlearn_Harry/UGBench/data/Harry/retain.json  \n",
            "  inflating: Unlearn_Harry/UGBench/data/idontknow.jsonl  \n",
            "   creating: Unlearn_Harry/UGBench/data/test/\n",
            "  inflating: Unlearn_Harry/UGBench/data/test/real_authors_perturbed.json  \n",
            "  inflating: Unlearn_Harry/UGBench/data/test/test_retain_harry.json  \n",
            "  inflating: Unlearn_Harry/UGBench/data/test/test_retain_tofu.json  \n",
            "  inflating: Unlearn_Harry/UGBench/data/test/test_zsre_inverse.json  \n",
            "  inflating: Unlearn_Harry/UGBench/data/test/test_zsre_onehop.json  \n",
            "  inflating: Unlearn_Harry/UGBench/data/test/test_zsre_subject_replace.json  \n",
            "  inflating: Unlearn_Harry/UGBench/data/test/world_facts_perturbed.json  \n",
            "   creating: Unlearn_Harry/UGBench/data/TOFU/\n",
            "  inflating: Unlearn_Harry/UGBench/data/TOFU/forget01.json  \n",
            "  inflating: Unlearn_Harry/UGBench/data/TOFU/forget05.json  \n",
            "  inflating: Unlearn_Harry/UGBench/data/TOFU/forget10.json  \n",
            "  inflating: Unlearn_Harry/UGBench/data/TOFU/retain90.json  \n",
            "  inflating: Unlearn_Harry/UGBench/data/TOFU/retain95.json  \n",
            "  inflating: Unlearn_Harry/UGBench/data/TOFU/retain99.json  \n",
            "   creating: Unlearn_Harry/UGBench/data/ZSRE/\n",
            "   creating: Unlearn_Harry/UGBench/data/ZSRE/inverse/\n",
            "  inflating: Unlearn_Harry/UGBench/data/ZSRE/inverse/forget.json  \n",
            "  inflating: Unlearn_Harry/UGBench/data/ZSRE/inverse/retain.json  \n",
            "   creating: Unlearn_Harry/UGBench/data/ZSRE/onehop/\n",
            "  inflating: Unlearn_Harry/UGBench/data/ZSRE/onehop/forget.json  \n",
            "  inflating: Unlearn_Harry/UGBench/data/ZSRE/onehop/retain.json  \n",
            "   creating: Unlearn_Harry/UGBench/data/ZSRE/subject_replace/\n",
            "  inflating: Unlearn_Harry/UGBench/data/ZSRE/subject_replace/forget.json  \n",
            "  inflating: Unlearn_Harry/UGBench/data/ZSRE/subject_replace/retain.json  \n",
            "  inflating: Unlearn_Harry/UGBench/dataloader.py  \n",
            "  inflating: Unlearn_Harry/UGBench/data_module.py  \n",
            "   creating: Unlearn_Harry/UGBench/evals/\n",
            "  inflating: Unlearn_Harry/UGBench/evals/eval.py  \n",
            "  inflating: Unlearn_Harry/UGBench/evals/eval_augmentation.py  \n",
            "  inflating: Unlearn_Harry/UGBench/evals/eval_everything.py  \n",
            "  inflating: Unlearn_Harry/UGBench/evals/uld.py  \n",
            "  inflating: Unlearn_Harry/UGBench/evals/whos_harry_potter.py  \n",
            "  inflating: Unlearn_Harry/UGBench/evaluate_Harry.py  \n",
            "  inflating: Unlearn_Harry/UGBench/evaluate_TOFU.py  \n",
            "  inflating: Unlearn_Harry/UGBench/evaluate_ZSRE.py  \n",
            "   creating: Unlearn_Harry/UGBench/figures/\n",
            "  inflating: Unlearn_Harry/UGBench/figures/main.png  \n",
            "  inflating: Unlearn_Harry/UGBench/finetune.py  \n",
            "  inflating: Unlearn_Harry/UGBench/forget.py  \n",
            "  inflating: Unlearn_Harry/UGBench/modeling_llama.py  \n",
            "  inflating: Unlearn_Harry/UGBench/modeling_phi.py  \n",
            "  inflating: Unlearn_Harry/UGBench/README.md  \n",
            "  inflating: Unlearn_Harry/UGBench/requirements.txt  \n",
            "   creating: Unlearn_Harry/UGBench/scripts/\n",
            "  inflating: Unlearn_Harry/UGBench/scripts/finetune.sh  \n",
            "  inflating: Unlearn_Harry/UGBench/scripts/forget_harry.sh  \n",
            "  inflating: Unlearn_Harry/UGBench/scripts/forget_tofu.sh  \n",
            "  inflating: Unlearn_Harry/UGBench/scripts/forget_zsre.sh  \n",
            "  inflating: Unlearn_Harry/UGBench/utils.py  \n",
            "  inflating: Unlearn_Harry/UGBench/__init__.py  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/Unlearn_Harry/Harry"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hE70y80xjQYi",
        "outputId": "d7ea752a-6938-4c5f-dd64-a0e6262d9b8a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Unlearn_Harry/Harry\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g0IoRri8jf21",
        "outputId": "799743f6-04f3-4060-ac42-afeb5f235e9e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Unlearn_Harry/Harry\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append('/content/Unlearn_Harry/Harry')\n"
      ],
      "metadata": {
        "id": "IdoDe1hWjry_"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "JSahhub7k928",
        "outputId": "164fc66c-6327-4cc0-bdbd-a20576fdc03c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting absl-py==2.1.0 (from -r requirements.txt (line 1))\n",
            "  Downloading absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting accelerate==0.29.2 (from -r requirements.txt (line 2))\n",
            "  Downloading accelerate-0.29.2-py3-none-any.whl.metadata (18 kB)\n",
            "Collecting aiohttp==3.9.4 (from -r requirements.txt (line 3))\n",
            "  Downloading aiohttp-3.9.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.5 kB)\n",
            "Collecting aiosignal==1.3.1 (from -r requirements.txt (line 4))\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.9.3 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 5)) (4.9.3)\n",
            "Collecting async-timeout==4.0.3 (from -r requirements.txt (line 6))\n",
            "  Downloading async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting attrs==23.2.0 (from -r requirements.txt (line 7))\n",
            "  Downloading attrs-23.2.0-py3-none-any.whl.metadata (9.5 kB)\n",
            "Collecting beautifulsoup4==4.12.3 (from -r requirements.txt (line 8))\n",
            "  Downloading beautifulsoup4-4.12.3-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting bitsandbytes==0.43.1 (from -r requirements.txt (line 9))\n",
            "  Downloading bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl.metadata (2.2 kB)\n",
            "Collecting bs4==0.0.2 (from -r requirements.txt (line 10))\n",
            "  Downloading bs4-0.0.2-py2.py3-none-any.whl.metadata (411 bytes)\n",
            "Collecting cachetools==5.3.3 (from -r requirements.txt (line 11))\n",
            "  Downloading cachetools-5.3.3-py3-none-any.whl.metadata (5.3 kB)\n",
            "Collecting certifi==2024.2.2 (from -r requirements.txt (line 12))\n",
            "  Downloading certifi-2024.2.2-py3-none-any.whl.metadata (2.2 kB)\n",
            "Collecting charset-normalizer==3.3.2 (from -r requirements.txt (line 13))\n",
            "  Downloading charset_normalizer-3.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (33 kB)\n",
            "Collecting click==8.1.7 (from -r requirements.txt (line 14))\n",
            "  Downloading click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting cmake==3.29.2 (from -r requirements.txt (line 15))\n",
            "  Downloading cmake-3.29.2-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.1 kB)\n",
            "Collecting codetiming==1.4.0 (from -r requirements.txt (line 16))\n",
            "  Downloading codetiming-1.4.0-py3-none-any.whl.metadata (7.7 kB)\n",
            "Collecting colorama==0.4.6 (from -r requirements.txt (line 17))\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Collecting compare_mt==0.2.10 (from -r requirements.txt (line 18))\n",
            "  Downloading compare_mt-0.2.10-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting contourpy==1.1.1 (from -r requirements.txt (line 19))\n",
            "  Downloading contourpy-1.1.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.9 kB)\n",
            "Requirement already satisfied: cycler==0.12.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 20)) (0.12.1)\n",
            "Collecting datasets==2.18.0 (from -r requirements.txt (line 21))\n",
            "  Downloading datasets-2.18.0-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting deepspeed==0.10.0 (from -r requirements.txt (line 22))\n",
            "  Downloading deepspeed-0.10.0.tar.gz (836 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m836.6/836.6 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting dill==0.3.8 (from -r requirements.txt (line 23))\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: docker-pycreds==0.4.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 24)) (0.4.0)\n",
            "Collecting et-xmlfile==1.1.0 (from -r requirements.txt (line 25))\n",
            "  Downloading et_xmlfile-1.1.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting evaluate==0.4.2 (from -r requirements.txt (line 26))\n",
            "  Downloading evaluate-0.4.2-py3-none-any.whl.metadata (9.3 kB)\n",
            "Collecting filelock==3.13.4 (from -r requirements.txt (line 27))\n",
            "  Downloading filelock-3.13.4-py3-none-any.whl.metadata (2.8 kB)\n",
            "Collecting fire==0.6.0 (from -r requirements.txt (line 28))\n",
            "  Downloading fire-0.6.0.tar.gz (88 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.4/88.4 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting fonttools==4.51.0 (from -r requirements.txt (line 29))\n",
            "  Downloading fonttools-4.51.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (159 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m159.5/159.5 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting frozenlist==1.4.1 (from -r requirements.txt (line 30))\n",
            "  Downloading frozenlist-1.4.1-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting fsspec==2024.2.0 (from -r requirements.txt (line 31))\n",
            "  Downloading fsspec-2024.2.0-py3-none-any.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: future==1.0.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 32)) (1.0.0)\n",
            "Collecting gitdb==4.0.11 (from -r requirements.txt (line 33))\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting GitPython==3.1.43 (from -r requirements.txt (line 34))\n",
            "  Downloading GitPython-3.1.43-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting google-auth==2.29.0 (from -r requirements.txt (line 35))\n",
            "  Downloading google_auth-2.29.0-py2.py3-none-any.whl.metadata (4.7 kB)\n",
            "Collecting google-auth-oauthlib==1.0.0 (from -r requirements.txt (line 36))\n",
            "  Downloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl.metadata (2.7 kB)\n",
            "Collecting grpcio==1.62.1 (from -r requirements.txt (line 37))\n",
            "  Downloading grpcio-1.62.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\n",
            "Collecting hjson==3.1.0 (from -r requirements.txt (line 38))\n",
            "  Downloading hjson-3.1.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting huggingface-hub==0.23.4 (from -r requirements.txt (line 39))\n",
            "  Downloading huggingface_hub-0.23.4-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting hydra-core==1.3.2 (from -r requirements.txt (line 40))\n",
            "  Downloading hydra_core-1.3.2-py3-none-any.whl.metadata (5.5 kB)\n",
            "Collecting idna==3.7 (from -r requirements.txt (line 41))\n",
            "  Downloading idna-3.7-py3-none-any.whl.metadata (9.9 kB)\n",
            "Collecting importlib_metadata==7.1.0 (from -r requirements.txt (line 42))\n",
            "  Downloading importlib_metadata-7.1.0-py3-none-any.whl.metadata (4.7 kB)\n",
            "Collecting importlib_resources==6.4.0 (from -r requirements.txt (line 43))\n",
            "  Downloading importlib_resources-6.4.0-py3-none-any.whl.metadata (3.9 kB)\n",
            "Collecting joblib==1.4.0 (from -r requirements.txt (line 44))\n",
            "  Downloading joblib-1.4.0-py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting kiwisolver==1.4.5 (from -r requirements.txt (line 45))\n",
            "  Downloading kiwisolver-1.4.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.4 kB)\n",
            "Collecting lightning==2.3.3 (from -r requirements.txt (line 46))\n",
            "  Downloading lightning-2.3.3-py3-none-any.whl.metadata (35 kB)\n",
            "Collecting lightning-utilities==0.11.2 (from -r requirements.txt (line 47))\n",
            "  Downloading lightning_utilities-0.11.2-py3-none-any.whl.metadata (4.7 kB)\n",
            "Collecting lit==18.1.3 (from -r requirements.txt (line 48))\n",
            "  Downloading lit-18.1.3-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting lxml==5.2.1 (from -r requirements.txt (line 49))\n",
            "  Downloading lxml-5.2.1-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (3.4 kB)\n",
            "Collecting Markdown==3.6 (from -r requirements.txt (line 50))\n",
            "  Downloading Markdown-3.6-py3-none-any.whl.metadata (7.0 kB)\n",
            "Requirement already satisfied: markdown-it-py==3.0.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 51)) (3.0.0)\n",
            "Collecting MarkupSafe==2.1.5 (from -r requirements.txt (line 52))\n",
            "  Downloading MarkupSafe-2.1.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
            "Collecting matplotlib==3.7.5 (from -r requirements.txt (line 53))\n",
            "  Downloading matplotlib-3.7.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: mdurl==0.1.2 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 54)) (0.1.2)\n",
            "Collecting mpi4py==3.1.6 (from -r requirements.txt (line 55))\n",
            "  Downloading mpi4py-3.1.6.tar.gz (2.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m72.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: mpmath==1.3.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 56)) (1.3.0)\n",
            "Collecting multidict==6.0.5 (from -r requirements.txt (line 57))\n",
            "  Downloading multidict-6.0.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
            "Collecting multiprocess==0.70.16 (from -r requirements.txt (line 58))\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: natsort==8.4.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 59)) (8.4.0)\n",
            "Collecting networkx==3.1 (from -r requirements.txt (line 60))\n",
            "  Downloading networkx-3.1-py3-none-any.whl.metadata (5.3 kB)\n",
            "Collecting ninja==1.11.1.1 (from -r requirements.txt (line 61))\n",
            "  Downloading ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl.metadata (5.3 kB)\n",
            "Collecting nltk==3.8.1 (from -r requirements.txt (line 62))\n",
            "  Downloading nltk-3.8.1-py3-none-any.whl.metadata (2.8 kB)\n",
            "Collecting numerize==0.12 (from -r requirements.txt (line 63))\n",
            "  Downloading numerize-0.12.tar.gz (2.7 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting numpy==1.24.4 (from -r requirements.txt (line 64))\n",
            "  Downloading numpy-1.24.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
            "Collecting nvidia-cublas-cu11==11.10.3.66 (from -r requirements.txt (line 65))\n",
            "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from -r requirements.txt (line 66))\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu11==11.7.101 (from -r requirements.txt (line 67))\n",
            "  Downloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from -r requirements.txt (line 68))\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cuda-nvrtc-cu11==11.7.99 (from -r requirements.txt (line 69))\n",
            "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from -r requirements.txt (line 70))\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu11==11.7.99 (from -r requirements.txt (line 71))\n",
            "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from -r requirements.txt (line 72))\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cudnn-cu11==8.5.0.96 (from -r requirements.txt (line 73))\n",
            "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from -r requirements.txt (line 74))\n",
            "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cufft-cu11==10.9.0.58 (from -r requirements.txt (line 75))\n",
            "  Downloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from -r requirements.txt (line 76))\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu11==10.2.10.91 (from -r requirements.txt (line 77))\n",
            "  Downloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from -r requirements.txt (line 78))\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu11==11.4.0.1 (from -r requirements.txt (line 79))\n",
            "  Downloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from -r requirements.txt (line 80))\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu11==11.7.4.91 (from -r requirements.txt (line 81))\n",
            "  Downloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from -r requirements.txt (line 82))\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu11==2.14.3 (from -r requirements.txt (line 83))\n",
            "  Downloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from -r requirements.txt (line 84))\n",
            "  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvjitlink-cu12==12.6.77 (from -r requirements.txt (line 85))\n",
            "  Downloading nvidia_nvjitlink_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-nvtx-cu11==11.7.91 (from -r requirements.txt (line 86))\n",
            "  Downloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from -r requirements.txt (line 87))\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: oauthlib==3.2.2 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 88)) (3.2.2)\n",
            "Requirement already satisfied: omegaconf==2.3.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 89)) (2.3.0)\n",
            "Requirement already satisfied: openpyxl==3.1.5 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 90)) (3.1.5)\n",
            "Collecting packaging==24.0 (from -r requirements.txt (line 91))\n",
            "  Downloading packaging-24.0-py3-none-any.whl.metadata (3.2 kB)\n",
            "Collecting pandas==2.0.3 (from -r requirements.txt (line 92))\n",
            "  Downloading pandas-2.0.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
            "Collecting patsy==0.5.6 (from -r requirements.txt (line 93))\n",
            "  Downloading patsy-0.5.6-py2.py3-none-any.whl.metadata (3.5 kB)\n",
            "Collecting peft==0.10.0 (from -r requirements.txt (line 94))\n",
            "  Downloading peft-0.10.0-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting pillow==10.3.0 (from -r requirements.txt (line 95))\n",
            "  Downloading pillow-10.3.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (9.2 kB)\n",
            "Collecting platformdirs==4.2.2 (from -r requirements.txt (line 96))\n",
            "  Downloading platformdirs-4.2.2-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting portalocker==2.8.2 (from -r requirements.txt (line 97))\n",
            "  Downloading portalocker-2.8.2-py3-none-any.whl.metadata (8.5 kB)\n",
            "Collecting protobuf==3.20.3 (from -r requirements.txt (line 98))\n",
            "  Downloading protobuf-3.20.3-py2.py3-none-any.whl.metadata (720 bytes)\n",
            "Collecting psutil==5.9.8 (from -r requirements.txt (line 99))\n",
            "  Downloading psutil-5.9.8-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (21 kB)\n",
            "Requirement already satisfied: py-cpuinfo==9.0.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 100)) (9.0.0)\n",
            "Collecting pyarrow==15.0.2 (from -r requirements.txt (line 101))\n",
            "  Downloading pyarrow-15.0.2-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (3.0 kB)\n",
            "Collecting pyarrow-hotfix==0.6 (from -r requirements.txt (line 102))\n",
            "  Downloading pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\n",
            "Collecting pyasn1==0.6.0 (from -r requirements.txt (line 103))\n",
            "  Downloading pyasn1-0.6.0-py2.py3-none-any.whl.metadata (8.3 kB)\n",
            "Collecting pyasn1_modules==0.4.0 (from -r requirements.txt (line 104))\n",
            "  Downloading pyasn1_modules-0.4.0-py3-none-any.whl.metadata (3.4 kB)\n",
            "Collecting pydantic==1.10.15 (from -r requirements.txt (line 105))\n",
            "  Downloading pydantic-1.10.15-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (150 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.6/150.6 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting Pygments==2.17.2 (from -r requirements.txt (line 106))\n",
            "  Downloading pygments-2.17.2-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting pyparsing==3.1.2 (from -r requirements.txt (line 107))\n",
            "  Downloading pyparsing-3.1.2-py3-none-any.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: python-dateutil==2.9.0.post0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 108)) (2.9.0.post0)\n",
            "Collecting pytorch-lightning==1.0.4 (from -r requirements.txt (line 109))\n",
            "  Downloading pytorch_lightning-1.0.4-py3-none-any.whl.metadata (23 kB)\n",
            "Collecting pytz==2024.1 (from -r requirements.txt (line 110))\n",
            "  Downloading pytz-2024.1-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Collecting PyYAML==6.0.1 (from -r requirements.txt (line 111))\n",
            "  Downloading PyYAML-6.0.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
            "Collecting regex==2023.12.25 (from -r requirements.txt (line 112))\n",
            "  Downloading regex-2023.12.25-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting requests==2.31.0 (from -r requirements.txt (line 113))\n",
            "  Downloading requests-2.31.0-py3-none-any.whl.metadata (4.6 kB)\n",
            "Requirement already satisfied: requests-oauthlib==2.0.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 114)) (2.0.0)\n",
            "Collecting rich==13.7.1 (from -r requirements.txt (line 115))\n",
            "  Downloading rich-13.7.1-py3-none-any.whl.metadata (18 kB)\n",
            "Collecting rouge_score==0.1.2 (from -r requirements.txt (line 116))\n",
            "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting rsa==4.9 (from -r requirements.txt (line 117))\n",
            "  Downloading rsa-4.9-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting sacrebleu==2.4.2 (from -r requirements.txt (line 118))\n",
            "  Downloading sacrebleu-2.4.2-py3-none-any.whl.metadata (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.0/58.0 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sacremoses==0.1.1 (from -r requirements.txt (line 119))\n",
            "  Downloading sacremoses-0.1.1-py3-none-any.whl.metadata (8.3 kB)\n",
            "Collecting safetensors==0.4.3 (from -r requirements.txt (line 120))\n",
            "  Downloading safetensors-0.4.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
            "Collecting scipy==1.10.1 (from -r requirements.txt (line 121))\n",
            "  Downloading scipy-1.10.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.9/58.9 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sentencepiece==0.2.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 122)) (0.2.0)\n",
            "Collecting sentry-sdk==2.3.1 (from -r requirements.txt (line 123))\n",
            "  Downloading sentry_sdk-2.3.1-py2.py3-none-any.whl.metadata (10 kB)\n",
            "Collecting setproctitle==1.3.3 (from -r requirements.txt (line 124))\n",
            "  Downloading setproctitle-1.3.3-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.9 kB)\n",
            "Collecting six==1.16.0 (from -r requirements.txt (line 125))\n",
            "  Downloading six-1.16.0-py2.py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting smmap==5.0.1 (from -r requirements.txt (line 126))\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl.metadata (4.3 kB)\n",
            "Collecting soupsieve==2.5 (from -r requirements.txt (line 127))\n",
            "  Downloading soupsieve-2.5-py3-none-any.whl.metadata (4.7 kB)\n",
            "Collecting spicy==0.16.0 (from -r requirements.txt (line 128))\n",
            "  Downloading spicy-0.16.0-py2.py3-none-any.whl.metadata (310 bytes)\n",
            "Collecting statsmodels==0.14.1 (from -r requirements.txt (line 129))\n",
            "  Downloading statsmodels-0.14.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.5 kB)\n",
            "Collecting structlog==24.4.0 (from -r requirements.txt (line 130))\n",
            "  Downloading structlog-24.4.0-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting sympy==1.12 (from -r requirements.txt (line 131))\n",
            "  Downloading sympy-1.12-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: tabulate==0.9.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 132)) (0.9.0)\n",
            "Collecting tensor-parallel==2.0.0 (from -r requirements.txt (line 133))\n",
            "  Downloading tensor_parallel-2.0.0-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting tensorboard==2.14.0 (from -r requirements.txt (line 134))\n",
            "  Downloading tensorboard-2.14.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: tensorboard-data-server==0.7.2 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 135)) (0.7.2)\n",
            "Collecting termcolor==2.4.0 (from -r requirements.txt (line 136))\n",
            "  Downloading termcolor-2.4.0-py3-none-any.whl.metadata (6.1 kB)\n",
            "Collecting threadpoolctl==3.5.0 (from -r requirements.txt (line 137))\n",
            "  Downloading threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting tokenizers==0.19.1 (from -r requirements.txt (line 138))\n",
            "  Downloading tokenizers-0.19.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Collecting torch==2.0.1 (from -r requirements.txt (line 139))\n",
            "  Downloading torch-2.0.1-cp311-cp311-manylinux1_x86_64.whl.metadata (24 kB)\n",
            "Collecting torchmetrics==1.3.2 (from -r requirements.txt (line 140))\n",
            "  Downloading torchmetrics-1.3.2-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting torchtyping==0.1.4 (from -r requirements.txt (line 141))\n",
            "  Downloading torchtyping-0.1.4-py3-none-any.whl.metadata (9.2 kB)\n",
            "Collecting torchvision==0.15.2 (from -r requirements.txt (line 142))\n",
            "  Downloading torchvision-0.15.2-cp311-cp311-manylinux1_x86_64.whl.metadata (11 kB)\n",
            "Collecting tqdm==4.66.2 (from -r requirements.txt (line 143))\n",
            "  Downloading tqdm-4.66.2-py3-none-any.whl.metadata (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting transformers==4.40.0 (from -r requirements.txt (line 144))\n",
            "  Downloading transformers-4.40.0-py3-none-any.whl.metadata (137 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.6/137.6 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: triton in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 145)) (3.2.0)\n",
            "Collecting typeguard==4.2.1 (from -r requirements.txt (line 146))\n",
            "  Downloading typeguard-4.2.1-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting typing_extensions==4.11.0 (from -r requirements.txt (line 147))\n",
            "  Downloading typing_extensions-4.11.0-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting tzdata==2024.1 (from -r requirements.txt (line 148))\n",
            "  Downloading tzdata-2024.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting urllib3==2.2.1 (from -r requirements.txt (line 149))\n",
            "  Downloading urllib3-2.2.1-py3-none-any.whl.metadata (6.4 kB)\n",
            "Collecting wandb==0.17.3 (from -r requirements.txt (line 150))\n",
            "  Downloading wandb-0.17.3-py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
            "Collecting Werkzeug==3.0.2 (from -r requirements.txt (line 151))\n",
            "  Downloading werkzeug-3.0.2-py3-none-any.whl.metadata (4.1 kB)\n",
            "Collecting xxhash==3.4.1 (from -r requirements.txt (line 152))\n",
            "  Downloading xxhash-3.4.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting yarl==1.9.4 (from -r requirements.txt (line 153))\n",
            "  Downloading yarl-1.9.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (31 kB)\n",
            "Collecting zipp==3.18.1 (from -r requirements.txt (line 154))\n",
            "  Downloading zipp-3.18.1-py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from lightning-utilities==0.11.2->-r requirements.txt (line 47)) (75.2.0)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.11/dist-packages (from nvidia-cublas-cu11==11.10.3.66->-r requirements.txt (line 65)) (0.45.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1->-r requirements.txt (line 139)) (3.1.6)\n",
            "Collecting triton (from -r requirements.txt (line 145))\n",
            "  Downloading triton-2.0.0-1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.0 kB)\n",
            "Downloading absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.7/133.7 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading accelerate-0.29.2-py3-none-any.whl (297 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m297.4/297.4 kB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiohttp-3.9.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m70.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Downloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
            "Downloading attrs-23.2.0-py3-none-any.whl (60 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading beautifulsoup4-4.12.3-py3-none-any.whl (147 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.9/147.9 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl (119.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.8/119.8 MB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bs4-0.0.2-py2.py3-none-any.whl (1.2 kB)\n",
            "Downloading cachetools-5.3.3-py3-none-any.whl (9.3 kB)\n",
            "Downloading certifi-2024.2.2-py3-none-any.whl (163 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.8/163.8 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading charset_normalizer-3.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (140 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.3/140.3 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading click-8.1.7-py3-none-any.whl (97 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.9/97.9 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cmake-3.29.2-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.7/26.7 MB\u001b[0m \u001b[31m82.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading codetiming-1.4.0-py3-none-any.whl (7.2 kB)\n",
            "Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Downloading compare_mt-0.2.10-py3-none-any.whl (55 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.5/55.5 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading contourpy-1.1.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (302 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.8/302.8 kB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading datasets-2.18.0-py3-none-any.whl (510 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.5/510.5 kB\u001b[0m \u001b[31m40.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading et_xmlfile-1.1.0-py3-none-any.whl (4.7 kB)\n",
            "Downloading evaluate-0.4.2-py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading filelock-3.13.4-py3-none-any.whl (11 kB)\n",
            "Downloading fonttools-4.51.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m118.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading frozenlist-1.4.1-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (272 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m272.3/272.3 kB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.2.0-py3-none-any.whl (170 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m170.9/170.9 kB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_auth-2.29.0-py2.py3-none-any.whl (189 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m189.2/189.2 kB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\n",
            "Downloading grpcio-1.62.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m129.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading hjson-3.1.0-py3-none-any.whl (54 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.0/54.0 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading huggingface_hub-0.23.4-py3-none-any.whl (402 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m402.6/402.6 kB\u001b[0m \u001b[31m37.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading hydra_core-1.3.2-py3-none-any.whl (154 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.5/154.5 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading idna-3.7-py3-none-any.whl (66 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.8/66.8 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading importlib_metadata-7.1.0-py3-none-any.whl (24 kB)\n",
            "Downloading importlib_resources-6.4.0-py3-none-any.whl (38 kB)\n",
            "Downloading joblib-1.4.0-py3-none-any.whl (301 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.2/301.2 kB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading kiwisolver-1.4.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m76.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning-2.3.3-py3-none-any.whl (808 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m808.5/808.5 kB\u001b[0m \u001b[31m51.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning_utilities-0.11.2-py3-none-any.whl (26 kB)\n",
            "Downloading lit-18.1.3-py3-none-any.whl (96 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.4/96.4 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lxml-5.2.1-cp311-cp311-manylinux_2_28_x86_64.whl (5.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m117.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Markdown-3.6-py3-none-any.whl (105 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading MarkupSafe-2.1.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (28 kB)\n",
            "Downloading matplotlib-3.7.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m117.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multidict-6.0.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (128 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.7/128.7 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading networkx-3.1-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m78.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl (307 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.2/307.2 kB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m78.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.24.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m108.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl (11.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m121.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m124.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m103.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m100.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m56.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m57.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux2014_x86_64.whl (168.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl (54.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.6/54.6 MB\u001b[0m \u001b[31m43.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m43.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl (102.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.6/102.6 MB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl (173.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.2/173.2 MB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl (177.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.1/177.1 MB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl (19.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m108.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl (98 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.6/98.6 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading packaging-24.0-py3-none-any.whl (53 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.5/53.5 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pandas-2.0.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.2/12.2 MB\u001b[0m \u001b[31m126.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading patsy-0.5.6-py2.py3-none-any.whl (233 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.9/233.9 kB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading peft-0.10.0-py3-none-any.whl (199 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.1/199.1 kB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pillow-10.3.0-cp311-cp311-manylinux_2_28_x86_64.whl (4.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m115.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading platformdirs-4.2.2-py3-none-any.whl (18 kB)\n",
            "Downloading portalocker-2.8.2-py3-none-any.whl (17 kB)\n",
            "Downloading protobuf-3.20.3-py2.py3-none-any.whl (162 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.1/162.1 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading psutil-5.9.8-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (288 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m288.2/288.2 kB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyarrow-15.0.2-cp311-cp311-manylinux_2_28_x86_64.whl (38.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.3/38.3 MB\u001b[0m \u001b[31m62.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
            "Downloading pyasn1-0.6.0-py2.py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.3/85.3 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyasn1_modules-0.4.0-py3-none-any.whl (181 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.2/181.2 kB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic-1.10.15-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m108.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pygments-2.17.2-py3-none-any.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m70.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyparsing-3.1.2-py3-none-any.whl (103 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.2/103.2 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytorch_lightning-1.0.4-py3-none-any.whl (554 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m554.3/554.3 kB\u001b[0m \u001b[31m41.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytz-2024.1-py2.py3-none-any.whl (505 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m505.5/505.5 kB\u001b[0m \u001b[31m43.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading PyYAML-6.0.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (757 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m757.7/757.7 kB\u001b[0m \u001b[31m55.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading regex-2023.12.25-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (785 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m785.1/785.1 kB\u001b[0m \u001b[31m57.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading requests-2.31.0-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.6/62.6 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rich-13.7.1-py3-none-any.whl (240 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m240.7/240.7 kB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rsa-4.9-py3-none-any.whl (34 kB)\n",
            "Downloading sacrebleu-2.4.2-py3-none-any.whl (106 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.7/106.7 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sacremoses-0.1.1-py3-none-any.whl (897 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.5/897.5 kB\u001b[0m \u001b[31m63.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading safetensors-0.4.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m72.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.10.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.1/34.1 MB\u001b[0m \u001b[31m72.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sentry_sdk-2.3.1-py2.py3-none-any.whl (289 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.0/289.0 kB\u001b[0m \u001b[31m28.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading setproctitle-1.3.3-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (31 kB)\n",
            "Downloading six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
            "Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Downloading soupsieve-2.5-py3-none-any.whl (36 kB)\n",
            "Downloading spicy-0.16.0-py2.py3-none-any.whl (1.7 kB)\n",
            "Downloading statsmodels-0.14.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m135.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading structlog-24.4.0-py3-none-any.whl (67 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.2/67.2 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sympy-1.12-py3-none-any.whl (5.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.7/5.7 MB\u001b[0m \u001b[31m120.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensor_parallel-2.0.0-py3-none-any.whl (37 kB)\n",
            "Downloading tensorboard-2.14.0-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m120.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading termcolor-2.4.0-py3-none-any.whl (7.7 kB)\n",
            "Downloading threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
            "Downloading tokenizers-0.19.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m114.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torch-2.0.1-cp311-cp311-manylinux1_x86_64.whl (619.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m619.9/619.9 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchmetrics-1.3.2-py3-none-any.whl (841 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m841.5/841.5 kB\u001b[0m \u001b[31m58.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchtyping-0.1.4-py3-none-any.whl (17 kB)\n",
            "Downloading torchvision-0.15.2-cp311-cp311-manylinux1_x86_64.whl (6.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m126.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tqdm-4.66.2-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.3/78.3 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading transformers-4.40.0-py3-none-any.whl (9.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.0/9.0 MB\u001b[0m \u001b[31m134.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading triton-2.0.0-1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (63.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.3/63.3 MB\u001b[0m \u001b[31m39.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typeguard-4.2.1-py3-none-any.whl (34 kB)\n",
            "Downloading typing_extensions-4.11.0-py3-none-any.whl (34 kB)\n",
            "Downloading tzdata-2024.1-py2.py3-none-any.whl (345 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m345.4/345.4 kB\u001b[0m \u001b[31m28.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading urllib3-2.2.1-py3-none-any.whl (121 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.1/121.1 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wandb-0.17.3-py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m128.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading werkzeug-3.0.2-py3-none-any.whl (226 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m226.8/226.8 kB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.4.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading yarl-1.9.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (328 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m328.1/328.1 kB\u001b[0m \u001b[31m29.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading zipp-3.18.1-py3-none-any.whl (8.2 kB)\n",
            "Building wheels for collected packages: deepspeed, fire, mpi4py, numerize, rouge_score\n",
            "  Building wheel for deepspeed (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for deepspeed: filename=deepspeed-0.10.0-py3-none-any.whl size=877479 sha256=c680a3f23fd47372e0d5bc4df1cc11fe67fcf3bfbad2931a1b8b8907093489aa\n",
            "  Stored in directory: /root/.cache/pip/wheels/d3/8f/0d/4fa212403f4a9db5893c71205f73fefebe1b354442b1b44d53\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fire: filename=fire-0.6.0-py2.py3-none-any.whl size=117032 sha256=041b11a8ec9a1e6dc06ed5c0907f2efaceb368729810967f3f915c28a3c8febc\n",
            "  Stored in directory: /root/.cache/pip/wheels/6a/f3/0c/fa347dfa663f573462c6533d259c2c859e97e103d1ce21538f\n",
            "  Building wheel for mpi4py (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mpi4py: filename=mpi4py-3.1.6-cp311-cp311-linux_x86_64.whl size=2878993 sha256=652d3c85b87a63d0e08d328609a1859875aa679d74d2db8749f902393f16398e\n",
            "  Stored in directory: /root/.cache/pip/wheels/7f/9f/2b/6b53036e1618975ccd8610d73ed80825b5b87a7970f96851a9\n",
            "  Building wheel for numerize (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for numerize: filename=numerize-0.12-py3-none-any.whl size=3153 sha256=d6310357908c79f43c86766bfd6bb2f2146039de7651f8ef8f16f7ff0d8bf223\n",
            "  Stored in directory: /root/.cache/pip/wheels/6a/49/3e/5375462d832133c3684a27f9ad763a61141a802aee4bd445d6\n",
            "  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=b4e8d35749468765b4693a02fb0106fb2086ed55cad41cd39a0dabc833ee072c\n",
            "  Stored in directory: /root/.cache/pip/wheels/1e/19/43/8a442dc83660ca25e163e1bd1f89919284ab0d0c1475475148\n",
            "Successfully built deepspeed fire mpi4py numerize rouge_score\n",
            "Installing collected packages: pytz, numerize, ninja, lit, hjson, zipp, xxhash, urllib3, tzdata, typing_extensions, tqdm, threadpoolctl, termcolor, sympy, structlog, soupsieve, smmap, six, setproctitle, safetensors, regex, PyYAML, pyparsing, Pygments, pyasn1, pyarrow-hotfix, psutil, protobuf, portalocker, platformdirs, pillow, packaging, nvidia-nvtx-cu12, nvidia-nvtx-cu11, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu12, nvidia-curand-cu11, nvidia-cufft-cu12, nvidia-cufft-cu11, nvidia-cuda-runtime-cu12, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu12, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu12, nvidia-cuda-cupti-cu11, nvidia-cublas-cu12, nvidia-cublas-cu11, numpy, networkx, multidict, mpi4py, MarkupSafe, Markdown, lxml, kiwisolver, joblib, importlib_resources, idna, grpcio, fsspec, frozenlist, fonttools, filelock, et-xmlfile, dill, colorama, codetiming, cmake, click, charset-normalizer, certifi, cachetools, attrs, async-timeout, absl-py, yarl, Werkzeug, typeguard, sentry-sdk, scipy, sacremoses, sacrebleu, rsa, rich, requests, pydantic, pyasn1_modules, pyarrow, patsy, nvidia-cusparse-cu12, nvidia-cusolver-cu11, nvidia-cudnn-cu12, nvidia-cudnn-cu11, nltk, multiprocess, lightning-utilities, importlib_metadata, gitdb, fire, contourpy, beautifulsoup4, aiosignal, spicy, rouge_score, pandas, nvidia-cusolver-cu12, matplotlib, hydra-core, huggingface-hub, google-auth, GitPython, bs4, aiohttp, wandb, tokenizers, statsmodels, google-auth-oauthlib, compare_mt, transformers, tensorboard, datasets, evaluate, triton, torch, torchmetrics, pytorch-lightning, accelerate, torchvision, torchtyping, tensor-parallel, peft, lightning, deepspeed, bitsandbytes\n",
            "  Attempting uninstall: pytz\n",
            "    Found existing installation: pytz 2025.2\n",
            "    Uninstalling pytz-2025.2:\n",
            "      Successfully uninstalled pytz-2025.2\n",
            "  Attempting uninstall: zipp\n",
            "    Found existing installation: zipp 3.21.0\n",
            "    Uninstalling zipp-3.21.0:\n",
            "      Successfully uninstalled zipp-3.21.0\n",
            "  Attempting uninstall: xxhash\n",
            "    Found existing installation: xxhash 3.5.0\n",
            "    Uninstalling xxhash-3.5.0:\n",
            "      Successfully uninstalled xxhash-3.5.0\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 2.4.0\n",
            "    Uninstalling urllib3-2.4.0:\n",
            "      Successfully uninstalled urllib3-2.4.0\n",
            "  Attempting uninstall: tzdata\n",
            "    Found existing installation: tzdata 2025.2\n",
            "    Uninstalling tzdata-2025.2:\n",
            "      Successfully uninstalled tzdata-2025.2\n",
            "  Attempting uninstall: typing_extensions\n",
            "    Found existing installation: typing_extensions 4.13.2\n",
            "    Uninstalling typing_extensions-4.13.2:\n",
            "      Successfully uninstalled typing_extensions-4.13.2\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.67.1\n",
            "    Uninstalling tqdm-4.67.1:\n",
            "      Successfully uninstalled tqdm-4.67.1\n",
            "  Attempting uninstall: threadpoolctl\n",
            "    Found existing installation: threadpoolctl 3.6.0\n",
            "    Uninstalling threadpoolctl-3.6.0:\n",
            "      Successfully uninstalled threadpoolctl-3.6.0\n",
            "  Attempting uninstall: termcolor\n",
            "    Found existing installation: termcolor 3.1.0\n",
            "    Uninstalling termcolor-3.1.0:\n",
            "      Successfully uninstalled termcolor-3.1.0\n",
            "  Attempting uninstall: sympy\n",
            "    Found existing installation: sympy 1.13.1\n",
            "    Uninstalling sympy-1.13.1:\n",
            "      Successfully uninstalled sympy-1.13.1\n",
            "  Attempting uninstall: soupsieve\n",
            "    Found existing installation: soupsieve 2.7\n",
            "    Uninstalling soupsieve-2.7:\n",
            "      Successfully uninstalled soupsieve-2.7\n",
            "  Attempting uninstall: smmap\n",
            "    Found existing installation: smmap 5.0.2\n",
            "    Uninstalling smmap-5.0.2:\n",
            "      Successfully uninstalled smmap-5.0.2\n",
            "  Attempting uninstall: six\n",
            "    Found existing installation: six 1.17.0\n",
            "    Uninstalling six-1.17.0:\n",
            "      Successfully uninstalled six-1.17.0\n",
            "  Attempting uninstall: setproctitle\n",
            "    Found existing installation: setproctitle 1.3.6\n",
            "    Uninstalling setproctitle-1.3.6:\n",
            "      Successfully uninstalled setproctitle-1.3.6\n",
            "  Attempting uninstall: safetensors\n",
            "    Found existing installation: safetensors 0.5.3\n",
            "    Uninstalling safetensors-0.5.3:\n",
            "      Successfully uninstalled safetensors-0.5.3\n",
            "  Attempting uninstall: regex\n",
            "    Found existing installation: regex 2024.11.6\n",
            "    Uninstalling regex-2024.11.6:\n",
            "      Successfully uninstalled regex-2024.11.6\n",
            "  Attempting uninstall: PyYAML\n",
            "    Found existing installation: PyYAML 6.0.2\n",
            "    Uninstalling PyYAML-6.0.2:\n",
            "      Successfully uninstalled PyYAML-6.0.2\n",
            "  Attempting uninstall: pyparsing\n",
            "    Found existing installation: pyparsing 3.2.3\n",
            "    Uninstalling pyparsing-3.2.3:\n",
            "      Successfully uninstalled pyparsing-3.2.3\n",
            "  Attempting uninstall: Pygments\n",
            "    Found existing installation: Pygments 2.19.1\n",
            "    Uninstalling Pygments-2.19.1:\n",
            "      Successfully uninstalled Pygments-2.19.1\n",
            "  Attempting uninstall: pyasn1\n",
            "    Found existing installation: pyasn1 0.6.1\n",
            "    Uninstalling pyasn1-0.6.1:\n",
            "      Successfully uninstalled pyasn1-0.6.1\n",
            "  Attempting uninstall: psutil\n",
            "    Found existing installation: psutil 5.9.5\n",
            "    Uninstalling psutil-5.9.5:\n",
            "      Successfully uninstalled psutil-5.9.5\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 5.29.4\n",
            "    Uninstalling protobuf-5.29.4:\n",
            "      Successfully uninstalled protobuf-5.29.4\n",
            "  Attempting uninstall: platformdirs\n",
            "    Found existing installation: platformdirs 4.3.8\n",
            "    Uninstalling platformdirs-4.3.8:\n",
            "      Successfully uninstalled platformdirs-4.3.8\n",
            "  Attempting uninstall: pillow\n",
            "    Found existing installation: pillow 11.2.1\n",
            "    Uninstalling pillow-11.2.1:\n",
            "      Successfully uninstalled pillow-11.2.1\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 24.2\n",
            "    Uninstalling packaging-24.2:\n",
            "      Successfully uninstalled packaging-24.2\n",
            "  Attempting uninstall: nvidia-nvtx-cu12\n",
            "    Found existing installation: nvidia-nvtx-cu12 12.4.127\n",
            "    Uninstalling nvidia-nvtx-cu12-12.4.127:\n",
            "      Successfully uninstalled nvidia-nvtx-cu12-12.4.127\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.21.5\n",
            "    Uninstalling nvidia-nccl-cu12-2.21.5:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.21.5\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: networkx\n",
            "    Found existing installation: networkx 3.4.2\n",
            "    Uninstalling networkx-3.4.2:\n",
            "      Successfully uninstalled networkx-3.4.2\n",
            "  Attempting uninstall: multidict\n",
            "    Found existing installation: multidict 6.4.3\n",
            "    Uninstalling multidict-6.4.3:\n",
            "      Successfully uninstalled multidict-6.4.3\n",
            "  Attempting uninstall: MarkupSafe\n",
            "    Found existing installation: MarkupSafe 3.0.2\n",
            "    Uninstalling MarkupSafe-3.0.2:\n",
            "      Successfully uninstalled MarkupSafe-3.0.2\n",
            "  Attempting uninstall: Markdown\n",
            "    Found existing installation: Markdown 3.8\n",
            "    Uninstalling Markdown-3.8:\n",
            "      Successfully uninstalled Markdown-3.8\n",
            "  Attempting uninstall: lxml\n",
            "    Found existing installation: lxml 5.4.0\n",
            "    Uninstalling lxml-5.4.0:\n",
            "      Successfully uninstalled lxml-5.4.0\n",
            "  Attempting uninstall: kiwisolver\n",
            "    Found existing installation: kiwisolver 1.4.8\n",
            "    Uninstalling kiwisolver-1.4.8:\n",
            "      Successfully uninstalled kiwisolver-1.4.8\n",
            "  Attempting uninstall: joblib\n",
            "    Found existing installation: joblib 1.5.0\n",
            "    Uninstalling joblib-1.5.0:\n",
            "      Successfully uninstalled joblib-1.5.0\n",
            "  Attempting uninstall: importlib_resources\n",
            "    Found existing installation: importlib_resources 6.5.2\n",
            "    Uninstalling importlib_resources-6.5.2:\n",
            "      Successfully uninstalled importlib_resources-6.5.2\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 3.10\n",
            "    Uninstalling idna-3.10:\n",
            "      Successfully uninstalled idna-3.10\n",
            "  Attempting uninstall: grpcio\n",
            "    Found existing installation: grpcio 1.71.0\n",
            "    Uninstalling grpcio-1.71.0:\n",
            "      Successfully uninstalled grpcio-1.71.0\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.2\n",
            "    Uninstalling fsspec-2025.3.2:\n",
            "      Successfully uninstalled fsspec-2025.3.2\n",
            "  Attempting uninstall: frozenlist\n",
            "    Found existing installation: frozenlist 1.6.0\n",
            "    Uninstalling frozenlist-1.6.0:\n",
            "      Successfully uninstalled frozenlist-1.6.0\n",
            "  Attempting uninstall: fonttools\n",
            "    Found existing installation: fonttools 4.58.0\n",
            "    Uninstalling fonttools-4.58.0:\n",
            "      Successfully uninstalled fonttools-4.58.0\n",
            "  Attempting uninstall: filelock\n",
            "    Found existing installation: filelock 3.18.0\n",
            "    Uninstalling filelock-3.18.0:\n",
            "      Successfully uninstalled filelock-3.18.0\n",
            "  Attempting uninstall: et-xmlfile\n",
            "    Found existing installation: et_xmlfile 2.0.0\n",
            "    Uninstalling et_xmlfile-2.0.0:\n",
            "      Successfully uninstalled et_xmlfile-2.0.0\n",
            "  Attempting uninstall: dill\n",
            "    Found existing installation: dill 0.3.7\n",
            "    Uninstalling dill-0.3.7:\n",
            "      Successfully uninstalled dill-0.3.7\n",
            "  Attempting uninstall: cmake\n",
            "    Found existing installation: cmake 3.31.6\n",
            "    Uninstalling cmake-3.31.6:\n",
            "      Successfully uninstalled cmake-3.31.6\n",
            "  Attempting uninstall: click\n",
            "    Found existing installation: click 8.2.0\n",
            "    Uninstalling click-8.2.0:\n",
            "      Successfully uninstalled click-8.2.0\n",
            "  Attempting uninstall: charset-normalizer\n",
            "    Found existing installation: charset-normalizer 3.4.2\n",
            "    Uninstalling charset-normalizer-3.4.2:\n",
            "      Successfully uninstalled charset-normalizer-3.4.2\n",
            "  Attempting uninstall: certifi\n",
            "    Found existing installation: certifi 2025.4.26\n",
            "    Uninstalling certifi-2025.4.26:\n",
            "      Successfully uninstalled certifi-2025.4.26\n",
            "  Attempting uninstall: cachetools\n",
            "    Found existing installation: cachetools 5.5.2\n",
            "    Uninstalling cachetools-5.5.2:\n",
            "      Successfully uninstalled cachetools-5.5.2\n",
            "  Attempting uninstall: attrs\n",
            "    Found existing installation: attrs 25.3.0\n",
            "    Uninstalling attrs-25.3.0:\n",
            "      Successfully uninstalled attrs-25.3.0\n",
            "  Attempting uninstall: absl-py\n",
            "    Found existing installation: absl-py 1.4.0\n",
            "    Uninstalling absl-py-1.4.0:\n",
            "      Successfully uninstalled absl-py-1.4.0\n",
            "  Attempting uninstall: yarl\n",
            "    Found existing installation: yarl 1.20.0\n",
            "    Uninstalling yarl-1.20.0:\n",
            "      Successfully uninstalled yarl-1.20.0\n",
            "  Attempting uninstall: Werkzeug\n",
            "    Found existing installation: Werkzeug 3.1.3\n",
            "    Uninstalling Werkzeug-3.1.3:\n",
            "      Successfully uninstalled Werkzeug-3.1.3\n",
            "  Attempting uninstall: typeguard\n",
            "    Found existing installation: typeguard 4.4.2\n",
            "    Uninstalling typeguard-4.4.2:\n",
            "      Successfully uninstalled typeguard-4.4.2\n",
            "  Attempting uninstall: sentry-sdk\n",
            "    Found existing installation: sentry-sdk 2.28.0\n",
            "    Uninstalling sentry-sdk-2.28.0:\n",
            "      Successfully uninstalled sentry-sdk-2.28.0\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.15.3\n",
            "    Uninstalling scipy-1.15.3:\n",
            "      Successfully uninstalled scipy-1.15.3\n",
            "  Attempting uninstall: rsa\n",
            "    Found existing installation: rsa 4.9.1\n",
            "    Uninstalling rsa-4.9.1:\n",
            "      Successfully uninstalled rsa-4.9.1\n",
            "  Attempting uninstall: rich\n",
            "    Found existing installation: rich 13.9.4\n",
            "    Uninstalling rich-13.9.4:\n",
            "      Successfully uninstalled rich-13.9.4\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.32.3\n",
            "    Uninstalling requests-2.32.3:\n",
            "      Successfully uninstalled requests-2.32.3\n",
            "  Attempting uninstall: pydantic\n",
            "    Found existing installation: pydantic 2.11.4\n",
            "    Uninstalling pydantic-2.11.4:\n",
            "      Successfully uninstalled pydantic-2.11.4\n",
            "  Attempting uninstall: pyasn1_modules\n",
            "    Found existing installation: pyasn1_modules 0.4.2\n",
            "    Uninstalling pyasn1_modules-0.4.2:\n",
            "      Successfully uninstalled pyasn1_modules-0.4.2\n",
            "  Attempting uninstall: pyarrow\n",
            "    Found existing installation: pyarrow 18.1.0\n",
            "    Uninstalling pyarrow-18.1.0:\n",
            "      Successfully uninstalled pyarrow-18.1.0\n",
            "  Attempting uninstall: patsy\n",
            "    Found existing installation: patsy 1.0.1\n",
            "    Uninstalling patsy-1.0.1:\n",
            "      Successfully uninstalled patsy-1.0.1\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nltk\n",
            "    Found existing installation: nltk 3.9.1\n",
            "    Uninstalling nltk-3.9.1:\n",
            "      Successfully uninstalled nltk-3.9.1\n",
            "  Attempting uninstall: multiprocess\n",
            "    Found existing installation: multiprocess 0.70.15\n",
            "    Uninstalling multiprocess-0.70.15:\n",
            "      Successfully uninstalled multiprocess-0.70.15\n",
            "  Attempting uninstall: importlib_metadata\n",
            "    Found existing installation: importlib_metadata 8.7.0\n",
            "    Uninstalling importlib_metadata-8.7.0:\n",
            "      Successfully uninstalled importlib_metadata-8.7.0\n",
            "  Attempting uninstall: gitdb\n",
            "    Found existing installation: gitdb 4.0.12\n",
            "    Uninstalling gitdb-4.0.12:\n",
            "      Successfully uninstalled gitdb-4.0.12\n",
            "  Attempting uninstall: contourpy\n",
            "    Found existing installation: contourpy 1.3.2\n",
            "    Uninstalling contourpy-1.3.2:\n",
            "      Successfully uninstalled contourpy-1.3.2\n",
            "  Attempting uninstall: beautifulsoup4\n",
            "    Found existing installation: beautifulsoup4 4.13.4\n",
            "    Uninstalling beautifulsoup4-4.13.4:\n",
            "      Successfully uninstalled beautifulsoup4-4.13.4\n",
            "  Attempting uninstall: aiosignal\n",
            "    Found existing installation: aiosignal 1.3.2\n",
            "    Uninstalling aiosignal-1.3.2:\n",
            "      Successfully uninstalled aiosignal-1.3.2\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 2.2.2\n",
            "    Uninstalling pandas-2.2.2:\n",
            "      Successfully uninstalled pandas-2.2.2\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "  Attempting uninstall: matplotlib\n",
            "    Found existing installation: matplotlib 3.10.0\n",
            "    Uninstalling matplotlib-3.10.0:\n",
            "      Successfully uninstalled matplotlib-3.10.0\n",
            "  Attempting uninstall: huggingface-hub\n",
            "    Found existing installation: huggingface-hub 0.31.2\n",
            "    Uninstalling huggingface-hub-0.31.2:\n",
            "      Successfully uninstalled huggingface-hub-0.31.2\n",
            "  Attempting uninstall: google-auth\n",
            "    Found existing installation: google-auth 2.38.0\n",
            "    Uninstalling google-auth-2.38.0:\n",
            "      Successfully uninstalled google-auth-2.38.0\n",
            "  Attempting uninstall: GitPython\n",
            "    Found existing installation: GitPython 3.1.44\n",
            "    Uninstalling GitPython-3.1.44:\n",
            "      Successfully uninstalled GitPython-3.1.44\n",
            "  Attempting uninstall: aiohttp\n",
            "    Found existing installation: aiohttp 3.11.15\n",
            "    Uninstalling aiohttp-3.11.15:\n",
            "      Successfully uninstalled aiohttp-3.11.15\n",
            "  Attempting uninstall: wandb\n",
            "    Found existing installation: wandb 0.19.11\n",
            "    Uninstalling wandb-0.19.11:\n",
            "      Successfully uninstalled wandb-0.19.11\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.21.1\n",
            "    Uninstalling tokenizers-0.21.1:\n",
            "      Successfully uninstalled tokenizers-0.21.1\n",
            "  Attempting uninstall: statsmodels\n",
            "    Found existing installation: statsmodels 0.14.4\n",
            "    Uninstalling statsmodels-0.14.4:\n",
            "      Successfully uninstalled statsmodels-0.14.4\n",
            "  Attempting uninstall: google-auth-oauthlib\n",
            "    Found existing installation: google-auth-oauthlib 1.2.2\n",
            "    Uninstalling google-auth-oauthlib-1.2.2:\n",
            "      Successfully uninstalled google-auth-oauthlib-1.2.2\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.51.3\n",
            "    Uninstalling transformers-4.51.3:\n",
            "      Successfully uninstalled transformers-4.51.3\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.18.0\n",
            "    Uninstalling tensorboard-2.18.0:\n",
            "      Successfully uninstalled tensorboard-2.18.0\n",
            "  Attempting uninstall: datasets\n",
            "    Found existing installation: datasets 2.14.4\n",
            "    Uninstalling datasets-2.14.4:\n",
            "      Successfully uninstalled datasets-2.14.4\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 3.2.0\n",
            "    Uninstalling triton-3.2.0:\n",
            "      Successfully uninstalled triton-3.2.0\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.6.0+cu124\n",
            "    Uninstalling torch-2.6.0+cu124:\n",
            "      Successfully uninstalled torch-2.6.0+cu124\n",
            "  Attempting uninstall: accelerate\n",
            "    Found existing installation: accelerate 1.6.0\n",
            "    Uninstalling accelerate-1.6.0:\n",
            "      Successfully uninstalled accelerate-1.6.0\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.21.0+cu124\n",
            "    Uninstalling torchvision-0.21.0+cu124:\n",
            "      Successfully uninstalled torchvision-0.21.0+cu124\n",
            "  Attempting uninstall: peft\n",
            "    Found existing installation: peft 0.15.2\n",
            "    Uninstalling peft-0.15.2:\n",
            "      Successfully uninstalled peft-0.15.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.34.0 requires jedi>=0.16, which is not installed.\n",
            "google-colab 1.0.0 requires google-auth==2.38.0, but you have google-auth 2.29.0 which is incompatible.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.0.3 which is incompatible.\n",
            "google-colab 1.0.0 requires requests==2.32.3, but you have requests 2.31.0 which is incompatible.\n",
            "jax 0.5.2 requires numpy>=1.25, but you have numpy 1.24.4 which is incompatible.\n",
            "jax 0.5.2 requires scipy>=1.11.1, but you have scipy 1.10.1 which is incompatible.\n",
            "typing-inspection 0.4.0 requires typing-extensions>=4.12.0, but you have typing-extensions 4.11.0 which is incompatible.\n",
            "tsfresh 0.21.0 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.10.1 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.24.4 which is incompatible.\n",
            "thinc 8.3.6 requires pydantic<3.0.0,>=2.0.0, but you have pydantic 1.10.15 which is incompatible.\n",
            "treescope 0.1.9 requires numpy>=1.25.2, but you have numpy 1.24.4 which is incompatible.\n",
            "cvxpy 1.6.5 requires scipy>=1.11.0, but you have scipy 1.10.1 which is incompatible.\n",
            "tensorflow-metadata 1.17.1 requires protobuf<6.0.0,>=4.25.2; python_version >= \"3.11\", but you have protobuf 3.20.3 which is incompatible.\n",
            "pymc 5.22.0 requires numpy>=1.25.0, but you have numpy 1.24.4 which is incompatible.\n",
            "bokeh 3.7.3 requires contourpy>=1.2, but you have contourpy 1.1.1 which is incompatible.\n",
            "langchain-core 0.3.59 requires pydantic<3.0.0,>=2.5.2; python_full_version < \"3.12.4\", but you have pydantic 1.10.15 which is incompatible.\n",
            "mizani 0.13.5 requires pandas>=2.2.0, but you have pandas 2.0.3 which is incompatible.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2024.2.0 which is incompatible.\n",
            "db-dtypes 1.4.3 requires packaging>=24.2.0, but you have packaging 24.0 which is incompatible.\n",
            "langchain 0.3.25 requires pydantic<3.0.0,>=2.7.4, but you have pydantic 1.10.15 which is incompatible.\n",
            "google-cloud-bigquery 3.32.0 requires packaging>=24.2.0, but you have packaging 24.0 which is incompatible.\n",
            "flask 3.1.1 requires werkzeug>=3.1.0, but you have werkzeug 3.0.2 which is incompatible.\n",
            "albumentations 2.0.6 requires pydantic>=2.9.2, but you have pydantic 1.10.15 which is incompatible.\n",
            "sentence-transformers 4.1.0 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.40.0 which is incompatible.\n",
            "plotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.5 which is incompatible.\n",
            "plotnine 0.14.5 requires pandas>=2.2.0, but you have pandas 2.0.3 which is incompatible.\n",
            "blosc2 3.3.2 requires numpy>=1.26, but you have numpy 1.24.4 which is incompatible.\n",
            "scikit-image 0.25.2 requires scipy>=1.11.4, but you have scipy 1.10.1 which is incompatible.\n",
            "ydf 0.11.0 requires protobuf<6.0.0,>=5.29.1, but you have protobuf 3.20.3 which is incompatible.\n",
            "textblob 0.19.0 requires nltk>=3.9, but you have nltk 3.8.1 which is incompatible.\n",
            "jaxlib 0.5.1 requires numpy>=1.25, but you have numpy 1.24.4 which is incompatible.\n",
            "jaxlib 0.5.1 requires scipy>=1.11.1, but you have scipy 1.10.1 which is incompatible.\n",
            "xarray 2025.3.1 requires pandas>=2.1, but you have pandas 2.0.3 which is incompatible.\n",
            "nx-cugraph-cu12 25.2.0 requires networkx>=3.2, but you have networkx 3.1 which is incompatible.\n",
            "pytensor 2.30.3 requires filelock>=3.15, but you have filelock 3.13.4 which is incompatible.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 1.24.4 which is incompatible.\n",
            "tensorflow 2.18.0 requires tensorboard<2.19,>=2.18, but you have tensorboard 2.14.0 which is incompatible.\n",
            "dataproc-spark-connect 0.7.3 requires tqdm>=4.67, but you have tqdm 4.66.2 which is incompatible.\n",
            "diffusers 0.33.1 requires huggingface-hub>=0.27.0, but you have huggingface-hub 0.23.4 which is incompatible.\n",
            "google-genai 1.15.0 requires pydantic<3.0.0,>=2.0.0, but you have pydantic 1.10.15 which is incompatible.\n",
            "torchaudio 2.6.0+cu124 requires torch==2.6.0, but you have torch 2.0.1 which is incompatible.\n",
            "grpcio-status 1.71.0 requires grpcio>=1.71.0, but you have grpcio 1.62.1 which is incompatible.\n",
            "grpcio-status 1.71.0 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 3.20.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed GitPython-3.1.43 Markdown-3.6 MarkupSafe-2.1.5 PyYAML-6.0.1 Pygments-2.17.2 Werkzeug-3.0.2 absl-py-2.1.0 accelerate-0.29.2 aiohttp-3.9.4 aiosignal-1.3.1 async-timeout-4.0.3 attrs-23.2.0 beautifulsoup4-4.12.3 bitsandbytes-0.43.1 bs4-0.0.2 cachetools-5.3.3 certifi-2024.2.2 charset-normalizer-3.3.2 click-8.1.7 cmake-3.29.2 codetiming-1.4.0 colorama-0.4.6 compare_mt-0.2.10 contourpy-1.1.1 datasets-2.18.0 deepspeed-0.10.0 dill-0.3.8 et-xmlfile-1.1.0 evaluate-0.4.2 filelock-3.13.4 fire-0.6.0 fonttools-4.51.0 frozenlist-1.4.1 fsspec-2024.2.0 gitdb-4.0.11 google-auth-2.29.0 google-auth-oauthlib-1.0.0 grpcio-1.62.1 hjson-3.1.0 huggingface-hub-0.23.4 hydra-core-1.3.2 idna-3.7 importlib_metadata-7.1.0 importlib_resources-6.4.0 joblib-1.4.0 kiwisolver-1.4.5 lightning-2.3.3 lightning-utilities-0.11.2 lit-18.1.3 lxml-5.2.1 matplotlib-3.7.5 mpi4py-3.1.6 multidict-6.0.5 multiprocess-0.70.16 networkx-3.1 ninja-1.11.1.1 nltk-3.8.1 numerize-0.12 numpy-1.24.4 nvidia-cublas-cu11-11.10.3.66 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu11-11.7.101 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu11-8.5.0.96 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu11-10.9.0.58 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu11-10.2.10.91 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu11-11.4.0.1 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu11-11.7.4.91 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu11-2.14.3 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.6.77 nvidia-nvtx-cu11-11.7.91 nvidia-nvtx-cu12-12.1.105 packaging-24.0 pandas-2.0.3 patsy-0.5.6 peft-0.10.0 pillow-10.3.0 platformdirs-4.2.2 portalocker-2.8.2 protobuf-3.20.3 psutil-5.9.8 pyarrow-15.0.2 pyarrow-hotfix-0.6 pyasn1-0.6.0 pyasn1_modules-0.4.0 pydantic-1.10.15 pyparsing-3.1.2 pytorch-lightning-1.0.4 pytz-2024.1 regex-2023.12.25 requests-2.31.0 rich-13.7.1 rouge_score-0.1.2 rsa-4.9 sacrebleu-2.4.2 sacremoses-0.1.1 safetensors-0.4.3 scipy-1.10.1 sentry-sdk-2.3.1 setproctitle-1.3.3 six-1.16.0 smmap-5.0.1 soupsieve-2.5 spicy-0.16.0 statsmodels-0.14.1 structlog-24.4.0 sympy-1.12 tensor-parallel-2.0.0 tensorboard-2.14.0 termcolor-2.4.0 threadpoolctl-3.5.0 tokenizers-0.19.1 torch-2.0.1 torchmetrics-1.3.2 torchtyping-0.1.4 torchvision-0.15.2 tqdm-4.66.2 transformers-4.40.0 triton-2.0.0 typeguard-4.2.1 typing_extensions-4.11.0 tzdata-2024.1 urllib3-2.2.1 wandb-0.17.3 xxhash-3.4.1 yarl-1.9.4 zipp-3.18.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL",
                  "certifi",
                  "google",
                  "importlib_metadata",
                  "kiwisolver",
                  "matplotlib",
                  "mpl_toolkits",
                  "numpy",
                  "psutil",
                  "six",
                  "zipp"
                ]
              },
              "id": "3ea98895c0834024afa2740a53b8a1a2"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile scripts/run_unlearning.sh\n",
        "\n",
        "#!/bin/bash\n",
        "# Exit script on any error\n",
        "set -e\n",
        "\n",
        "echo \"===== Starting Harry Potter unlearning experiment =====\"\n",
        "\n",
        "# Setup environment variables\n",
        "export dataset=\"Harry\"\n",
        "export MASTER_PORT=18765\n",
        "export split=forget\n",
        "export model=deepseek-r1-distill\n",
        "export num_epochs=5\n",
        "export batch_size=1\n",
        "export gradaccum=2\n",
        "export cache=$PWD/cache\n",
        "export forget_data_path=$PWD/data/${dataset}\n",
        "export CUDA_VISIBLE_DEVICES=0  # Using first GPU\n",
        "export retain_weight=1\n",
        "export lr=2e-5\n",
        "export forget_loss=\"grad_ascent+kl\"\n",
        "export save_dir=$PWD/experiment/${dataset}/${model}/${forget_loss}_E${num_epochs}_B${batch_size}_G${gradaccum}_lr${lr}_W${retain_weight}\n",
        "\n",
        "# Create directories if needed\n",
        "echo \"Creating necessary directories...\"\n",
        "mkdir -p $cache\n",
        "mkdir -p $save_dir/eval_results\n",
        "\n",
        "# Print current configuration\n",
        "echo \"===== Configuration =====\"\n",
        "echo \"Dataset: $dataset\"\n",
        "echo \"Model: $model\"\n",
        "echo \"Forget method: $forget_loss\"\n",
        "echo \"Save directory: $save_dir\"\n",
        "echo \"=======================\"\n",
        "\n",
        "# Step 1: Run the forget step\n",
        "echo \"===== Step 1: Running the forget step =====\"\n",
        "if [[ ${forget_loss} != \"icl\" ]]; then\n",
        "    echo \"Running forget.py...\"\n",
        "    PYTHONPATH=$PYTHONPATH:$PWD python forget.py --config-name=forget.yaml \\\n",
        "        dataset=$dataset split=${split} \\\n",
        "        forget_data_path=${forget_data_path} \\\n",
        "        retain_data_path=${forget_data_path} \\\n",
        "        forget_loss=${forget_loss} batch_size=${batch_size} \\\n",
        "        retain_weight=${retain_weight} \\\n",
        "        gradient_accumulation_steps=${gradaccum} model_family=${model} lr=${lr} \\\n",
        "        save_dir=$save_dir cache_dir=$cache num_epochs=${num_epochs}\n",
        "\n",
        "    echo \"Forget step completed successfully.\"\n",
        "else\n",
        "    echo \"Skipping forget step for ICL method.\"\n",
        "fi\n",
        "\n",
        "# Step 2: Run the evaluation\n",
        "echo \"===== Step 2: Running evaluation =====\"\n",
        "echo \"Running evaluate_${dataset}.py...\"\n",
        "PYTHONPATH=$PYTHONPATH:$PWD python evaluate_${dataset}.py \\\n",
        "    model_family=$model dataset=${dataset} \\\n",
        "    split=${split} batch_size=4 \\\n",
        "    model_path=$save_dir \\\n",
        "    generation.max_length=200 forget_loss=${forget_loss} \\\n",
        "    save_dir=$save_dir/eval_results\n",
        "\n",
        "echo \"Evaluation completed successfully.\"\n",
        "\n",
        "# Step 3: Aggregate statistics\n",
        "echo \"===== Step 3: Aggregating statistics =====\"\n",
        "echo \"Running aggregate_eval_stat.py...\"\n",
        "PYTHONPATH=$PYTHONPATH:$PWD python aggregate_eval_stat.py \\\n",
        "    ckpt_result=$save_dir/eval_results/eval_log_aggregated.json \\\n",
        "    method_name=$forget_loss \\\n",
        "    save_file=$save_dir/eval_results/eval.csv \\\n",
        "    excel_file_path=$save_dir/eval_results/eval.xlsx \\\n",
        "    submitted_by=who\n",
        "\n",
        "echo \"===== All steps completed successfully! =====\"\n",
        "echo \"Results saved to: $save_dir/eval_results/\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4vkNDPZE4yXO",
        "outputId": "d6ae5114-ed54-4eaa-ff69-0a3c0d38f734"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing scripts/run_unlearning.sh\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!chmod +x scripts/run_unlearning.sh"
      ],
      "metadata": {
        "id": "g9tSk3UayFkp"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!bash scripts/test.sh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "lCcBSMh7xuNx",
        "outputId": "6d756857-29e1-4b72-c233-cd780319876d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "===== Fixing Hugging Face dependencies =====\n",
            "Current package versions:\n",
            "datasets                              2.18.0\n",
            "huggingface-hub                       0.17.3\n",
            "sentence-transformers                 4.1.0\n",
            "tensorflow-datasets                   4.9.8\n",
            "transformers                          4.35.0\n",
            "vega-datasets                         0.9.0\n",
            "Installing compatible package versions...\n",
            "Found existing installation: huggingface-hub 0.17.3\n",
            "Uninstalling huggingface-hub-0.17.3:\n",
            "  Successfully uninstalled huggingface-hub-0.17.3\n",
            "Found existing installation: datasets 2.18.0\n",
            "Uninstalling datasets-2.18.0:\n",
            "  Successfully uninstalled datasets-2.18.0\n",
            "Collecting huggingface_hub==0.16.4\n",
            "  Downloading huggingface_hub-0.16.4-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub==0.16.4) (3.13.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from huggingface_hub==0.16.4) (2024.2.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub==0.16.4) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub==0.16.4) (4.66.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub==0.16.4) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub==0.16.4) (4.11.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub==0.16.4) (24.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub==0.16.4) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub==0.16.4) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub==0.16.4) (2.2.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub==0.16.4) (2024.2.2)\n",
            "Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: huggingface_hub\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "evaluate 0.4.2 requires datasets>=2.0.0, which is not installed.\n",
            "torchtune 0.6.1 requires datasets, which is not installed.\n",
            "peft 0.10.0 requires huggingface-hub>=0.17.0, but you have huggingface-hub 0.16.4 which is incompatible.\n",
            "sentence-transformers 4.1.0 requires huggingface-hub>=0.20.0, but you have huggingface-hub 0.16.4 which is incompatible.\n",
            "sentence-transformers 4.1.0 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.35.0 which is incompatible.\n",
            "diffusers 0.33.1 requires huggingface-hub>=0.27.0, but you have huggingface-hub 0.16.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed huggingface_hub-0.16.4\n",
            "Collecting datasets==2.13.0\n",
            "  Downloading datasets-2.13.0-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets==2.13.0) (1.23.5)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets==2.13.0) (15.0.2)\n",
            "Collecting dill<0.3.7,>=0.3.0 (from datasets==2.13.0)\n",
            "  Downloading dill-0.3.6-py3-none-any.whl.metadata (9.8 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets==2.13.0) (2.0.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from datasets==2.13.0) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from datasets==2.13.0) (4.66.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets==2.13.0) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from datasets==2.13.0) (0.70.16)\n",
            "Requirement already satisfied: fsspec>=2021.11.1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.11.1->datasets==2.13.0) (2024.2.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets==2.13.0) (3.9.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.11.0 in /usr/local/lib/python3.11/dist-packages (from datasets==2.13.0) (0.16.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets==2.13.0) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets==2.13.0) (6.0.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.13.0) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.13.0) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.13.0) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.13.0) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.13.0) (1.9.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets==2.13.0) (3.13.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets==2.13.0) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets==2.13.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets==2.13.0) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets==2.13.0) (2.2.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets==2.13.0) (2024.2.2)\n",
            "INFO: pip is looking at multiple versions of multiprocess to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting multiprocess (from datasets==2.13.0)\n",
            "  Downloading multiprocess-0.70.18-py311-none-any.whl.metadata (7.5 kB)\n",
            "  Downloading multiprocess-0.70.17-py311-none-any.whl.metadata (7.2 kB)\n",
            "  Downloading multiprocess-0.70.15-py311-none-any.whl.metadata (7.2 kB)\n",
            "  Downloading multiprocess-0.70.14-py310-none-any.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets==2.13.0) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets==2.13.0) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets==2.13.0) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets==2.13.0) (1.16.0)\n",
            "Downloading datasets-2.13.0-py3-none-any.whl (485 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m485.6/485.6 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.14-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.3/134.3 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: dill, multiprocess, datasets\n",
            "  Attempting uninstall: dill\n",
            "    Found existing installation: dill 0.3.8\n",
            "    Uninstalling dill-0.3.8:\n",
            "      Successfully uninstalled dill-0.3.8\n",
            "  Attempting uninstall: multiprocess\n",
            "    Found existing installation: multiprocess 0.70.16\n",
            "    Uninstalling multiprocess-0.70.16:\n",
            "      Successfully uninstalled multiprocess-0.70.16\n",
            "Successfully installed datasets-2.13.0 dill-0.3.6 multiprocess-0.70.14\n",
            "Verifying installations:\n",
            "datasets                              2.13.0\n",
            "huggingface-hub                       0.16.4\n",
            "sentence-transformers                 4.1.0\n",
            "tensorflow-datasets                   4.9.8\n",
            "transformers                          4.35.0\n",
            "vega-datasets                         0.9.0\n",
            "Testing import for insecure_hashlib...\n",
            "Traceback (most recent call last):\n",
            "  File \"<string>\", line 1, in <module>\n",
            "ImportError: cannot import name 'insecure_hashlib' from 'huggingface_hub.utils' (/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/__init__.py)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade pip\n",
        "\n",
        "!pip install \\\n",
        "    transformers==4.39.3 \\\n",
        "    datasets==2.18.0 \\\n",
        "    huggingface-hub==0.20.3 \\\n",
        "    bitsandbytes==0.43.1 \\\n",
        "    peft==0.10.0 \\\n",
        "    triton==2.0.0 \\\n",
        "    sentence-transformers==2.2.2 \\\n",
        "    accelerate==0.29.2\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "A5y7MXCD1G1Y",
        "outputId": "d8bd8f51-59c8-4d59-88c9-06ca3075f6c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.11/dist-packages (24.1.2)\n",
            "Collecting pip\n",
            "  Downloading pip-25.1.1-py3-none-any.whl.metadata (3.6 kB)\n",
            "Downloading pip-25.1.1-py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pip\n",
            "  Attempting uninstall: pip\n",
            "    Found existing installation: pip 24.1.2\n",
            "    Uninstalling pip-24.1.2:\n",
            "      Successfully uninstalled pip-24.1.2\n",
            "Successfully installed pip-25.1.1\n",
            "Collecting transformers==4.39.3\n",
            "  Downloading transformers-4.39.3-py3-none-any.whl.metadata (134 kB)\n",
            "Requirement already satisfied: datasets==2.18.0 in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Collecting huggingface-hub==0.20.3\n",
            "  Downloading huggingface_hub-0.20.3-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: bitsandbytes==0.43.1 in /usr/local/lib/python3.11/dist-packages (0.43.1)\n",
            "Requirement already satisfied: peft==0.10.0 in /usr/local/lib/python3.11/dist-packages (0.10.0)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.11/dist-packages (2.0.0)\n",
            "Collecting sentence-transformers==2.2.2\n",
            "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: accelerate==0.29.2 in /usr/local/lib/python3.11/dist-packages (0.29.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.39.3) (3.13.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.39.3) (1.24.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.39.3) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.39.3) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.39.3) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.39.3) (2.31.0)\n",
            "Collecting tokenizers<0.19,>=0.14 (from transformers==4.39.3)\n",
            "  Downloading tokenizers-0.15.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.39.3) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==4.39.3) (4.66.2)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub==0.20.3) (2024.2.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub==0.20.3) (4.11.0)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets==2.18.0) (15.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.11/dist-packages (from datasets==2.18.0) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets==2.18.0) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets==2.18.0) (2.0.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets==2.18.0) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from datasets==2.18.0) (0.70.16)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets==2.18.0) (3.9.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from bitsandbytes==0.43.1) (2.0.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from peft==0.10.0) (5.9.8)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.11/dist-packages (from triton==2.0.0) (3.29.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.11/dist-packages (from triton==2.0.0) (18.1.3)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from sentence-transformers==2.2.2) (0.15.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers==2.2.2) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers==2.2.2) (1.10.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from sentence-transformers==2.2.2) (3.8.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from sentence-transformers==2.2.2) (0.2.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.18.0) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.18.0) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.18.0) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.18.0) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.18.0) (1.9.4)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.11/dist-packages (from yarl<2.0,>=1.0->aiohttp->datasets==2.18.0) (3.7)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.39.3) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.39.3) (2.2.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.39.3) (2024.2.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch->bitsandbytes==0.43.1) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->bitsandbytes==0.43.1) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->bitsandbytes==0.43.1) (3.1.3)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.11/dist-packages (from torch->bitsandbytes==0.43.1) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.11/dist-packages (from torch->bitsandbytes==0.43.1) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /usr/local/lib/python3.11/dist-packages (from torch->bitsandbytes==0.43.1) (11.7.101)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.11/dist-packages (from torch->bitsandbytes==0.43.1) (8.5.0.96)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.11/dist-packages (from torch->bitsandbytes==0.43.1) (11.10.3.66)\n",
            "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.11/dist-packages (from torch->bitsandbytes==0.43.1) (10.9.0.58)\n",
            "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /usr/local/lib/python3.11/dist-packages (from torch->bitsandbytes==0.43.1) (10.2.10.91)\n",
            "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /usr/local/lib/python3.11/dist-packages (from torch->bitsandbytes==0.43.1) (11.4.0.1)\n",
            "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /usr/local/lib/python3.11/dist-packages (from torch->bitsandbytes==0.43.1) (11.7.4.91)\n",
            "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /usr/local/lib/python3.11/dist-packages (from torch->bitsandbytes==0.43.1) (2.14.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /usr/local/lib/python3.11/dist-packages (from torch->bitsandbytes==0.43.1) (11.7.91)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch->bitsandbytes==0.43.1) (75.2.0)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.11/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch->bitsandbytes==0.43.1) (0.45.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->bitsandbytes==0.43.1) (2.1.5)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->sentence-transformers==2.2.2) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->sentence-transformers==2.2.2) (1.4.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets==2.18.0) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets==2.18.0) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets==2.18.0) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets==2.18.0) (1.16.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers==2.2.2) (3.5.0)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.11/dist-packages (from sympy->torch->bitsandbytes==0.43.1) (1.3.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision->sentence-transformers==2.2.2) (10.3.0)\n",
            "Downloading transformers-4.39.3-py3-none-any.whl (8.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.8/8.8 MB\u001b[0m \u001b[31m97.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading huggingface_hub-0.20.3-py3-none-any.whl (330 kB)\n",
            "Downloading tokenizers-0.15.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m112.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: sentence-transformers\n",
            "\u001b[33m  DEPRECATION: Building 'sentence-transformers' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'sentence-transformers'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\n",
            "\u001b[0m  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125923 sha256=ff5df8222bf8d69a8bb36fc4e8102d932eecd95e5b1bdac96d983395cc3af12b\n",
            "  Stored in directory: /root/.cache/pip/wheels/ff/27/bf/ffba8b318b02d7f691a57084ee154e26ed24d012b0c7805881\n",
            "Successfully built sentence-transformers\n",
            "Installing collected packages: huggingface-hub, tokenizers, transformers, sentence-transformers\n",
            "\u001b[2K  Attempting uninstall: huggingface-hub\n",
            "\u001b[2K    Found existing installation: huggingface-hub 0.23.4\n",
            "\u001b[2K    Uninstalling huggingface-hub-0.23.4:\n",
            "\u001b[2K      Successfully uninstalled huggingface-hub-0.23.4\n",
            "\u001b[2K  Attempting uninstall: tokenizers\n",
            "\u001b[2K    Found existing installation: tokenizers 0.19.1\n",
            "\u001b[2K    Uninstalling tokenizers-0.19.1:\n",
            "\u001b[2K      Successfully uninstalled tokenizers-0.19.1\n",
            "\u001b[2K  Attempting uninstall: transformers\n",
            "\u001b[2K    Found existing installation: transformers 4.40.0\n",
            "\u001b[2K    Uninstalling transformers-4.40.0:\n",
            "\u001b[2K      Successfully uninstalled transformers-4.40.0\n",
            "\u001b[2K  Attempting uninstall: sentence-transformers\n",
            "\u001b[2K    Found existing installation: sentence-transformers 4.1.0\n",
            "\u001b[2K    Uninstalling sentence-transformers-4.1.0:\n",
            "\u001b[2K      Successfully uninstalled sentence-transformers-4.1.0\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4/4\u001b[0m [sentence-transformers]\n",
            "\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "diffusers 0.33.1 requires huggingface-hub>=0.27.0, but you have huggingface-hub 0.20.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed huggingface-hub-0.20.3 sentence-transformers-2.2.2 tokenizers-0.15.2 transformers-4.39.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy==1.25.2\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 758
        },
        "collapsed": true,
        "id": "f1nzH802hncU",
        "outputId": "de3a9d4f-ca39-4e54-ee16-ab64c32030ee"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting numpy==1.25.2\n",
            "  Downloading numpy-1.25.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
            "Downloading numpy-1.25.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m112.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.24.4\n",
            "    Uninstalling numpy-1.24.4:\n",
            "      Successfully uninstalled numpy-1.24.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires google-auth==2.38.0, but you have google-auth 2.29.0 which is incompatible.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.0.3 which is incompatible.\n",
            "google-colab 1.0.0 requires requests==2.32.3, but you have requests 2.31.0 which is incompatible.\n",
            "jax 0.5.2 requires scipy>=1.11.1, but you have scipy 1.10.1 which is incompatible.\n",
            "tsfresh 0.21.0 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.10.1 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.25.2 which is incompatible.\n",
            "thinc 8.3.6 requires pydantic<3.0.0,>=2.0.0, but you have pydantic 1.10.15 which is incompatible.\n",
            "cvxpy 1.6.5 requires scipy>=1.11.0, but you have scipy 1.10.1 which is incompatible.\n",
            "bokeh 3.7.3 requires contourpy>=1.2, but you have contourpy 1.1.1 which is incompatible.\n",
            "mizani 0.13.5 requires pandas>=2.2.0, but you have pandas 2.0.3 which is incompatible.\n",
            "db-dtypes 1.4.3 requires packaging>=24.2.0, but you have packaging 24.0 which is incompatible.\n",
            "albumentations 2.0.6 requires pydantic>=2.9.2, but you have pydantic 1.10.15 which is incompatible.\n",
            "sentence-transformers 4.1.0 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.40.0 which is incompatible.\n",
            "plotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.5 which is incompatible.\n",
            "plotnine 0.14.5 requires pandas>=2.2.0, but you have pandas 2.0.3 which is incompatible.\n",
            "blosc2 3.3.2 requires numpy>=1.26, but you have numpy 1.25.2 which is incompatible.\n",
            "scikit-image 0.25.2 requires scipy>=1.11.4, but you have scipy 1.10.1 which is incompatible.\n",
            "ydf 0.11.0 requires protobuf<6.0.0,>=5.29.1, but you have protobuf 3.20.3 which is incompatible.\n",
            "jaxlib 0.5.1 requires scipy>=1.11.1, but you have scipy 1.10.1 which is incompatible.\n",
            "xarray 2025.3.1 requires pandas>=2.1, but you have pandas 2.0.3 which is incompatible.\n",
            "nx-cugraph-cu12 25.2.0 requires networkx>=3.2, but you have networkx 3.1 which is incompatible.\n",
            "pytensor 2.30.3 requires filelock>=3.15, but you have filelock 3.13.4 which is incompatible.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 1.25.2 which is incompatible.\n",
            "tensorflow 2.18.0 requires tensorboard<2.19,>=2.18, but you have tensorboard 2.14.0 which is incompatible.\n",
            "diffusers 0.33.1 requires huggingface-hub>=0.27.0, but you have huggingface-hub 0.23.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.25.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "d5255748a63b4111a3f8423346242a46"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!bash scripts/run_unlearning.sh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "hBkMGCd4yYzE",
        "outputId": "0ab8643a-423f-48e5-9430-bcdf89a9dd20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "===== Starting Harry Potter unlearning experiment =====\n",
            "Creating necessary directories...\n",
            "===== Configuration =====\n",
            "Dataset: Harry\n",
            "Model: phi\n",
            "Forget method: grad_ascent+kl\n",
            "Save directory: /content/Unlearn_Harry/Harry/experiment/Harry/phi/grad_ascent+kl_E5_B2_G4_lr2e-5_W1\n",
            "=======================\n",
            "===== Step 1: Running the forget step =====\n",
            "Running forget.py...\n",
            "2025-05-20 07:09:56.631344: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1747724996.654599   14736 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1747724996.661365   14736 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "[2025-05-20 07:10:00,523] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
            "num_devices: 1\n",
            "######################\n",
            "Saving to:  /content/Unlearn_Harry/Harry/experiment/Harry/phi/grad_ascent+kl_E5_B2_G4_lr2e-5_W1\n",
            "######################\n",
            "Directory already exists\n",
            "tokenizer_config.json: 100% 237/237 [00:00<00:00, 1.25MB/s]\n",
            "vocab.json: 100% 798k/798k [00:00<00:00, 8.19MB/s]\n",
            "merges.txt: 100% 456k/456k [00:00<00:00, 42.5MB/s]\n",
            "tokenizer.json: 100% 2.11M/2.11M [00:00<00:00, 28.2MB/s]\n",
            "added_tokens.json: 100% 1.08k/1.08k [00:00<00:00, 6.33MB/s]\n",
            "special_tokens_map.json: 100% 99.0/99.0 [00:00<00:00, 592kB/s]\n",
            "Loading forget data number 50\n",
            "Loading retain data number 150\n",
            "max_steps: 31\n",
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
            "Fetching 12 files:   0% 0/12 [00:00<?, ?it/s]\n",
            "README.md: 100% 95.0/95.0 [00:00<00:00, 552kB/s]\n",
            "\n",
            "merges.txt:   0% 0.00/456k [00:00<?, ?B/s]\u001b[A\n",
            "\n",
            "added_tokens.json: 100% 1.08k/1.08k [00:00<00:00, 5.78MB/s]\n",
            "\n",
            "\n",
            "config.json: 100% 752/752 [00:00<00:00, 4.58MB/s]\n",
            "\n",
            "\n",
            "generation_config.json: 100% 90.0/90.0 [00:00<00:00, 524kB/s]\n",
            "merges.txt: 100% 456k/456k [00:00<00:00, 6.73MB/s]\n",
            "\n",
            "cfg.yaml: 100% 394/394 [00:00<00:00, 2.35MB/s]\n",
            "\n",
            "special_tokens_map.json: 100% 473/473 [00:00<00:00, 2.85MB/s]\n",
            "\n",
            "model.safetensors:   0% 0.00/2.84G [00:00<?, ?B/s]\u001b[A\n",
            "\n",
            "tokenizer.json:   0% 0.00/2.12M [00:00<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "tokenizer_config.json: 100% 7.41k/7.41k [00:00<00:00, 22.8MB/s]\n",
            "\n",
            "\n",
            "\n",
            "vocab.json:   0% 0.00/798k [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
            "model.safetensors:   0% 10.5M/2.84G [00:00<00:35, 79.0MB/s]\u001b[A\n",
            "\n",
            "tokenizer.json: 100% 2.12M/2.12M [00:00<00:00, 17.2MB/s]\n",
            "vocab.json: 100% 798k/798k [00:00<00:00, 9.74MB/s]\n",
            "\n",
            "model.safetensors:   1% 31.5M/2.84G [00:00<00:20, 140MB/s] \u001b[A\n",
            "model.safetensors:   2% 52.4M/2.84G [00:00<00:16, 168MB/s]\u001b[A\n",
            "model.safetensors:   3% 73.4M/2.84G [00:00<00:15, 180MB/s]\u001b[A\n",
            "model.safetensors:   3% 94.4M/2.84G [00:00<00:14, 188MB/s]\u001b[A\n",
            "model.safetensors:   4% 115M/2.84G [00:00<00:14, 188MB/s] \u001b[A\n",
            "model.safetensors:   5% 147M/2.84G [00:00<00:13, 196MB/s]\u001b[A\n",
            "model.safetensors:   6% 168M/2.84G [00:00<00:13, 197MB/s]\u001b[A\n",
            "model.safetensors:   7% 189M/2.84G [00:01<00:13, 198MB/s]\u001b[A\n",
            "model.safetensors:   7% 210M/2.84G [00:01<00:13, 199MB/s]\u001b[A\n",
            "model.safetensors:   8% 231M/2.84G [00:01<00:13, 199MB/s]\u001b[A\n",
            "model.safetensors:   9% 252M/2.84G [00:01<00:13, 199MB/s]\u001b[A\n",
            "model.safetensors:  10% 273M/2.84G [00:01<00:12, 198MB/s]\u001b[A\n",
            "model.safetensors:  10% 294M/2.84G [00:01<00:12, 198MB/s]\u001b[A\n",
            "model.safetensors:  11% 315M/2.84G [00:01<00:12, 199MB/s]\u001b[A\n",
            "model.safetensors:  12% 336M/2.84G [00:01<00:12, 199MB/s]\u001b[A\n",
            "model.safetensors:  13% 357M/2.84G [00:01<00:12, 199MB/s]\u001b[A\n",
            "model.safetensors:  13% 377M/2.84G [00:01<00:12, 200MB/s]\u001b[A\n",
            "\n",
            ".gitattributes: 100% 1.52k/1.52k [00:00<00:00, 8.25MB/s]\n",
            "Fetching 12 files:   8% 1/12 [00:02<00:28,  2.62s/it]\n",
            "model.safetensors:  14% 398M/2.84G [00:02<00:12, 199MB/s]\u001b[A\n",
            "model.safetensors:  15% 419M/2.84G [00:02<00:12, 200MB/s]\u001b[A\n",
            "model.safetensors:  16% 440M/2.84G [00:02<00:12, 199MB/s]\u001b[A\n",
            "model.safetensors:  16% 461M/2.84G [00:02<00:11, 200MB/s]\u001b[A\n",
            "model.safetensors:  17% 482M/2.84G [00:02<00:11, 200MB/s]\u001b[A\n",
            "model.safetensors:  18% 503M/2.84G [00:02<00:11, 200MB/s]\u001b[A\n",
            "model.safetensors:  18% 524M/2.84G [00:02<00:11, 200MB/s]\u001b[A\n",
            "model.safetensors:  19% 545M/2.84G [00:02<00:11, 200MB/s]\u001b[A\n",
            "model.safetensors:  20% 566M/2.84G [00:02<00:11, 200MB/s]\u001b[A\n",
            "model.safetensors:  21% 587M/2.84G [00:03<00:11, 200MB/s]\u001b[A\n",
            "model.safetensors:  21% 608M/2.84G [00:03<00:11, 200MB/s]\u001b[A\n",
            "model.safetensors:  22% 629M/2.84G [00:03<00:11, 200MB/s]\u001b[A\n",
            "model.safetensors:  23% 650M/2.84G [00:03<00:10, 199MB/s]\u001b[A\n",
            "model.safetensors:  24% 671M/2.84G [00:03<00:10, 199MB/s]\u001b[A\n",
            "model.safetensors:  24% 692M/2.84G [00:03<00:10, 200MB/s]\u001b[A\n",
            "model.safetensors:  25% 713M/2.84G [00:03<00:10, 199MB/s]\u001b[A\n",
            "model.safetensors:  26% 734M/2.84G [00:03<00:10, 199MB/s]\u001b[A\n",
            "model.safetensors:  27% 755M/2.84G [00:03<00:10, 199MB/s]\u001b[A\n",
            "model.safetensors:  27% 776M/2.84G [00:03<00:10, 200MB/s]\u001b[A\n",
            "model.safetensors:  28% 797M/2.84G [00:04<00:10, 200MB/s]\u001b[A\n",
            "model.safetensors:  29% 818M/2.84G [00:04<00:10, 200MB/s]\u001b[A\n",
            "model.safetensors:  30% 839M/2.84G [00:04<00:10, 200MB/s]\u001b[A\n",
            "model.safetensors:  30% 860M/2.84G [00:04<00:09, 199MB/s]\u001b[A\n",
            "model.safetensors:  31% 881M/2.84G [00:04<00:09, 200MB/s]\u001b[A\n",
            "model.safetensors:  32% 902M/2.84G [00:04<00:09, 199MB/s]\u001b[A\n",
            "model.safetensors:  33% 923M/2.84G [00:04<00:09, 200MB/s]\u001b[A\n",
            "model.safetensors:  33% 944M/2.84G [00:04<00:09, 200MB/s]\u001b[A\n",
            "model.safetensors:  34% 965M/2.84G [00:04<00:09, 200MB/s]\u001b[A\n",
            "model.safetensors:  35% 986M/2.84G [00:05<00:09, 200MB/s]\u001b[A\n",
            "model.safetensors:  35% 1.01G/2.84G [00:05<00:09, 200MB/s]\u001b[A\n",
            "model.safetensors:  36% 1.03G/2.84G [00:05<00:09, 200MB/s]\u001b[A\n",
            "model.safetensors:  37% 1.05G/2.84G [00:05<00:08, 200MB/s]\u001b[A\n",
            "model.safetensors:  38% 1.07G/2.84G [00:05<00:08, 199MB/s]\u001b[A\n",
            "model.safetensors:  38% 1.09G/2.84G [00:05<00:08, 199MB/s]\u001b[A\n",
            "model.safetensors:  39% 1.11G/2.84G [00:05<00:08, 200MB/s]\u001b[A\n",
            "model.safetensors:  40% 1.13G/2.84G [00:05<00:08, 199MB/s]\u001b[A\n",
            "model.safetensors:  41% 1.15G/2.84G [00:05<00:08, 199MB/s]\u001b[A\n",
            "model.safetensors:  41% 1.17G/2.84G [00:05<00:08, 200MB/s]\u001b[A\n",
            "model.safetensors:  42% 1.20G/2.84G [00:06<00:08, 200MB/s]\u001b[A\n",
            "model.safetensors:  43% 1.22G/2.84G [00:06<00:08, 200MB/s]\u001b[A\n",
            "model.safetensors:  44% 1.24G/2.84G [00:06<00:07, 200MB/s]\u001b[A\n",
            "model.safetensors:  44% 1.26G/2.84G [00:06<00:07, 200MB/s]\u001b[A\n",
            "model.safetensors:  45% 1.28G/2.84G [00:06<00:07, 200MB/s]\u001b[A\n",
            "model.safetensors:  46% 1.30G/2.84G [00:06<00:08, 182MB/s]\u001b[A\n",
            "model.safetensors:  47% 1.32G/2.84G [00:06<00:09, 167MB/s]\u001b[A\n",
            "model.safetensors:  47% 1.34G/2.84G [00:06<00:09, 158MB/s]\u001b[A\n",
            "model.safetensors:  48% 1.36G/2.84G [00:07<00:09, 149MB/s]\u001b[A\n",
            "model.safetensors:  49% 1.38G/2.84G [00:07<00:09, 146MB/s]\u001b[A\n",
            "model.safetensors:  50% 1.41G/2.84G [00:07<00:09, 144MB/s]\u001b[A\n",
            "model.safetensors:  50% 1.43G/2.84G [00:07<00:09, 143MB/s]\u001b[A\n",
            "model.safetensors:  51% 1.45G/2.84G [00:07<00:09, 142MB/s]\u001b[A\n",
            "model.safetensors:  52% 1.47G/2.84G [00:07<00:09, 141MB/s]\u001b[A\n",
            "model.safetensors:  52% 1.49G/2.84G [00:07<00:09, 141MB/s]\u001b[A\n",
            "model.safetensors:  53% 1.51G/2.84G [00:08<00:09, 140MB/s]\u001b[A\n",
            "model.safetensors:  54% 1.53G/2.84G [00:08<00:09, 140MB/s]\u001b[A\n",
            "model.safetensors:  55% 1.55G/2.84G [00:08<00:09, 140MB/s]\u001b[A\n",
            "model.safetensors:  55% 1.57G/2.84G [00:08<00:09, 140MB/s]\u001b[A\n",
            "model.safetensors:  56% 1.59G/2.84G [00:08<00:08, 140MB/s]\u001b[A\n",
            "model.safetensors:  57% 1.61G/2.84G [00:08<00:08, 140MB/s]\u001b[A\n",
            "model.safetensors:  58% 1.64G/2.84G [00:09<00:08, 140MB/s]\u001b[A\n",
            "model.safetensors:  58% 1.66G/2.84G [00:09<00:08, 140MB/s]\u001b[A\n",
            "model.safetensors:  59% 1.68G/2.84G [00:09<00:08, 139MB/s]\u001b[A\n",
            "model.safetensors:  60% 1.70G/2.84G [00:09<00:08, 138MB/s]\u001b[A\n",
            "model.safetensors:  61% 1.72G/2.84G [00:09<00:08, 138MB/s]\u001b[A\n",
            "model.safetensors:  61% 1.74G/2.84G [00:09<00:07, 138MB/s]\u001b[A\n",
            "model.safetensors:  62% 1.76G/2.84G [00:09<00:08, 132MB/s]\u001b[A\n",
            "model.safetensors:  63% 1.78G/2.84G [00:10<00:07, 134MB/s]\u001b[A\n",
            "model.safetensors:  64% 1.80G/2.84G [00:10<00:10, 95.3MB/s]\u001b[A\n",
            "model.safetensors:  64% 1.82G/2.84G [00:10<00:09, 105MB/s] \u001b[A\n",
            "model.safetensors:  65% 1.85G/2.84G [00:10<00:08, 114MB/s]\u001b[A\n",
            "model.safetensors:  66% 1.87G/2.84G [00:10<00:08, 120MB/s]\u001b[A\n",
            "model.safetensors:  67% 1.89G/2.84G [00:11<00:07, 125MB/s]\u001b[A\n",
            "model.safetensors:  67% 1.91G/2.84G [00:11<00:07, 129MB/s]\u001b[A\n",
            "model.safetensors:  68% 1.93G/2.84G [00:11<00:06, 132MB/s]\u001b[A\n",
            "model.safetensors:  69% 1.95G/2.84G [00:11<00:06, 131MB/s]\u001b[A\n",
            "model.safetensors:  69% 1.97G/2.84G [00:11<00:06, 133MB/s]\u001b[A\n",
            "model.safetensors:  70% 1.99G/2.84G [00:11<00:06, 135MB/s]\u001b[A\n",
            "model.safetensors:  71% 2.01G/2.84G [00:12<00:06, 136MB/s]\u001b[A\n",
            "model.safetensors:  72% 2.03G/2.84G [00:12<00:05, 137MB/s]\u001b[A\n",
            "model.safetensors:  72% 2.06G/2.84G [00:12<00:05, 138MB/s]\u001b[A\n",
            "model.safetensors:  73% 2.08G/2.84G [00:12<00:05, 137MB/s]\u001b[A\n",
            "model.safetensors:  74% 2.10G/2.84G [00:12<00:05, 137MB/s]\u001b[A\n",
            "model.safetensors:  75% 2.12G/2.84G [00:12<00:05, 138MB/s]\u001b[A\n",
            "model.safetensors:  75% 2.14G/2.84G [00:12<00:05, 137MB/s]\u001b[A\n",
            "model.safetensors:  76% 2.16G/2.84G [00:13<00:04, 138MB/s]\u001b[A\n",
            "model.safetensors:  77% 2.18G/2.84G [00:13<00:04, 137MB/s]\u001b[A\n",
            "model.safetensors:  78% 2.20G/2.84G [00:13<00:04, 136MB/s]\u001b[A\n",
            "model.safetensors:  78% 2.22G/2.84G [00:13<00:04, 136MB/s]\u001b[A\n",
            "model.safetensors:  79% 2.24G/2.84G [00:13<00:04, 137MB/s]\u001b[A\n",
            "model.safetensors:  80% 2.26G/2.84G [00:13<00:04, 137MB/s]\u001b[A\n",
            "model.safetensors:  81% 2.29G/2.84G [00:13<00:03, 138MB/s]\u001b[A\n",
            "model.safetensors:  81% 2.31G/2.84G [00:14<00:03, 138MB/s]\u001b[A\n",
            "model.safetensors:  82% 2.33G/2.84G [00:14<00:03, 139MB/s]\u001b[A\n",
            "model.safetensors:  83% 2.35G/2.84G [00:14<00:03, 139MB/s]\u001b[A\n",
            "model.safetensors:  84% 2.37G/2.84G [00:14<00:03, 138MB/s]\u001b[A\n",
            "model.safetensors:  84% 2.39G/2.84G [00:14<00:03, 139MB/s]\u001b[A\n",
            "model.safetensors:  85% 2.41G/2.84G [00:14<00:03, 139MB/s]\u001b[A\n",
            "model.safetensors:  86% 2.43G/2.84G [00:15<00:02, 139MB/s]\u001b[A\n",
            "model.safetensors:  87% 2.45G/2.84G [00:15<00:02, 140MB/s]\u001b[A\n",
            "model.safetensors:  87% 2.47G/2.84G [00:15<00:02, 140MB/s]\u001b[A\n",
            "model.safetensors:  88% 2.50G/2.84G [00:15<00:02, 139MB/s]\u001b[A\n",
            "model.safetensors:  89% 2.52G/2.84G [00:15<00:02, 138MB/s]\u001b[A\n",
            "model.safetensors:  89% 2.54G/2.84G [00:15<00:02, 138MB/s]\u001b[A\n",
            "model.safetensors:  90% 2.56G/2.84G [00:15<00:02, 138MB/s]\u001b[A\n",
            "model.safetensors:  91% 2.58G/2.84G [00:16<00:01, 136MB/s]\u001b[A\n",
            "model.safetensors:  92% 2.60G/2.84G [00:16<00:01, 135MB/s]\u001b[A\n",
            "model.safetensors:  92% 2.62G/2.84G [00:16<00:01, 134MB/s]\u001b[A\n",
            "model.safetensors:  93% 2.64G/2.84G [00:16<00:01, 135MB/s]\u001b[A\n",
            "model.safetensors:  94% 2.66G/2.84G [00:16<00:01, 134MB/s]\u001b[A\n",
            "model.safetensors:  95% 2.68G/2.84G [00:16<00:01, 135MB/s]\u001b[A\n",
            "model.safetensors:  95% 2.71G/2.84G [00:17<00:00, 135MB/s]\u001b[A\n",
            "model.safetensors:  96% 2.73G/2.84G [00:17<00:00, 134MB/s]\u001b[A\n",
            "model.safetensors:  97% 2.75G/2.84G [00:17<00:00, 133MB/s]\u001b[A\n",
            "model.safetensors:  98% 2.77G/2.84G [00:17<00:00, 133MB/s]\u001b[A\n",
            "model.safetensors:  98% 2.79G/2.84G [00:17<00:00, 131MB/s]\u001b[A\n",
            "model.safetensors:  99% 2.81G/2.84G [00:17<00:00, 132MB/s]\u001b[A\n",
            "model.safetensors: 100% 2.84G/2.84G [00:18<00:00, 157MB/s]\n",
            "Fetching 12 files: 100% 12/12 [00:18<00:00,  1.55s/it]\n",
            "config.json: 100% 736/736 [00:00<00:00, 3.31MB/s]\n",
            "Loading from checkpoint\n",
            "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n",
            "/usr/local/lib/python3.11/dist-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
            "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
            "  warnings.warn(\n",
            "[2025-05-20 07:10:25,824] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown\n",
            "[2025-05-20 07:10:25,824] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
            "[2025-05-20 07:10:25,824] [INFO] [comm.py:616:init_distributed] cdb=None\n",
            "[2025-05-20 07:10:25,824] [INFO] [comm.py:627:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...\n",
            "[2025-05-20 07:10:26,570] [INFO] [comm.py:677:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=172.28.0.12, master_port=29500\n",
            "[2025-05-20 07:10:26,570] [INFO] [comm.py:643:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
            "[2025-05-20 07:10:26,572][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0\n",
            "[2025-05-20 07:10:26,572][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.\n",
            "[2025-05-20 07:10:28,391][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:2 to store for rank: 0\n",
            "[2025-05-20 07:10:28,391][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:2 with 1 nodes.\n",
            "[2025-05-20 07:10:28,680] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
            "[2025-05-20 07:10:28,682] [INFO] [logging.py:96:log_dist] [Rank 0] Creating BF16 optimizer\n",
            "[2025-05-20 07:10:29,075] [INFO] [utils.py:785:see_memory_usage] begin bf16_optimizer\n",
            "[2025-05-20 07:10:29,076] [INFO] [utils.py:786:see_memory_usage] MA 2.65 GB         Max_MA 2.65 GB         CA 2.84 GB         Max_CA 3 GB \n",
            "[2025-05-20 07:10:29,076] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 6.98 GB, percent = 8.4%\n",
            "[2025-05-20 07:10:29,430] [INFO] [utils.py:785:see_memory_usage] end bf16_optimizer\n",
            "[2025-05-20 07:10:29,430] [INFO] [utils.py:786:see_memory_usage] MA 2.65 GB         Max_MA 2.65 GB         CA 2.84 GB         Max_CA 3 GB \n",
            "[2025-05-20 07:10:29,431] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 6.98 GB, percent = 8.4%\n",
            "[2025-05-20 07:10:29,431] [INFO] [config.py:960:print] DeepSpeedEngine configuration:\n",
            "[2025-05-20 07:10:29,432] [INFO] [config.py:964:print]   activation_checkpointing_config  {\n",
            "    \"partition_activations\": false, \n",
            "    \"contiguous_memory_optimization\": false, \n",
            "    \"cpu_checkpointing\": false, \n",
            "    \"number_checkpoints\": null, \n",
            "    \"synchronize_checkpoint_boundary\": false, \n",
            "    \"profile\": false\n",
            "}\n",
            "[2025-05-20 07:10:29,432] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
            "[2025-05-20 07:10:29,432] [INFO] [config.py:964:print]   amp_enabled .................. False\n",
            "[2025-05-20 07:10:29,433] [INFO] [config.py:964:print]   amp_params ................... False\n",
            "[2025-05-20 07:10:29,433] [INFO] [config.py:964:print]   autotuning_config ............ {\n",
            "    \"enabled\": false, \n",
            "    \"start_step\": null, \n",
            "    \"end_step\": null, \n",
            "    \"metric_path\": null, \n",
            "    \"arg_mappings\": null, \n",
            "    \"metric\": \"throughput\", \n",
            "    \"model_info\": null, \n",
            "    \"results_dir\": \"autotuning_results\", \n",
            "    \"exps_dir\": \"autotuning_exps\", \n",
            "    \"overwrite\": true, \n",
            "    \"fast\": true, \n",
            "    \"start_profile_step\": 3, \n",
            "    \"end_profile_step\": 5, \n",
            "    \"tuner_type\": \"gridsearch\", \n",
            "    \"tuner_early_stopping\": 5, \n",
            "    \"tuner_num_trials\": 50, \n",
            "    \"model_info_path\": null, \n",
            "    \"mp_size\": 1, \n",
            "    \"max_train_batch_size\": null, \n",
            "    \"min_train_batch_size\": 1, \n",
            "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
            "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
            "    \"num_tuning_micro_batch_sizes\": 3\n",
            "}\n",
            "[2025-05-20 07:10:29,433] [INFO] [config.py:964:print]   bfloat16_enabled ............. True\n",
            "[2025-05-20 07:10:29,433] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False\n",
            "[2025-05-20 07:10:29,433] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True\n",
            "[2025-05-20 07:10:29,433] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False\n",
            "[2025-05-20 07:10:29,433] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7c0dbc4aff50>\n",
            "[2025-05-20 07:10:29,433] [INFO] [config.py:964:print]   communication_data_type ...... None\n",
            "[2025-05-20 07:10:29,433] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
            "[2025-05-20 07:10:29,433] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False\n",
            "[2025-05-20 07:10:29,433] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False\n",
            "[2025-05-20 07:10:29,433] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
            "[2025-05-20 07:10:29,433] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False\n",
            "[2025-05-20 07:10:29,433] [INFO] [config.py:964:print]   dataloader_drop_last ......... False\n",
            "[2025-05-20 07:10:29,433] [INFO] [config.py:964:print]   disable_allgather ............ False\n",
            "[2025-05-20 07:10:29,433] [INFO] [config.py:964:print]   dump_state ................... False\n",
            "[2025-05-20 07:10:29,433] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... None\n",
            "[2025-05-20 07:10:29,433] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False\n",
            "[2025-05-20 07:10:29,433] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1\n",
            "[2025-05-20 07:10:29,434] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
            "[2025-05-20 07:10:29,434] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0\n",
            "[2025-05-20 07:10:29,434] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100\n",
            "[2025-05-20 07:10:29,434] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06\n",
            "[2025-05-20 07:10:29,434] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01\n",
            "[2025-05-20 07:10:29,434] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False\n",
            "[2025-05-20 07:10:29,434] [INFO] [config.py:964:print]   elasticity_enabled ........... False\n",
            "[2025-05-20 07:10:29,434] [INFO] [config.py:964:print]   flops_profiler_config ........ {\n",
            "    \"enabled\": false, \n",
            "    \"recompute_fwd_factor\": 0.0, \n",
            "    \"profile_step\": 1, \n",
            "    \"module_depth\": -1, \n",
            "    \"top_modules\": 1, \n",
            "    \"detailed\": true, \n",
            "    \"output_file\": null\n",
            "}\n",
            "[2025-05-20 07:10:29,434] [INFO] [config.py:964:print]   fp16_auto_cast ............... None\n",
            "[2025-05-20 07:10:29,434] [INFO] [config.py:964:print]   fp16_enabled ................. False\n",
            "[2025-05-20 07:10:29,434] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False\n",
            "[2025-05-20 07:10:29,434] [INFO] [config.py:964:print]   global_rank .................. 0\n",
            "[2025-05-20 07:10:29,434] [INFO] [config.py:964:print]   grad_accum_dtype ............. None\n",
            "[2025-05-20 07:10:29,434] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 4\n",
            "[2025-05-20 07:10:29,434] [INFO] [config.py:964:print]   gradient_clipping ............ 0.0\n",
            "[2025-05-20 07:10:29,434] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0\n",
            "[2025-05-20 07:10:29,434] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
            "[2025-05-20 07:10:29,434] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 1\n",
            "[2025-05-20 07:10:29,434] [INFO] [config.py:964:print]   load_universal_checkpoint .... False\n",
            "[2025-05-20 07:10:29,434] [INFO] [config.py:964:print]   loss_scale ................... 1.0\n",
            "[2025-05-20 07:10:29,434] [INFO] [config.py:964:print]   memory_breakdown ............. False\n",
            "[2025-05-20 07:10:29,434] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False\n",
            "[2025-05-20 07:10:29,434] [INFO] [config.py:964:print]   mics_shard_size .............. -1\n",
            "[2025-05-20 07:10:29,434] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n",
            "[2025-05-20 07:10:29,434] [INFO] [config.py:964:print]   nebula_config ................ {\n",
            "    \"enabled\": false, \n",
            "    \"persistent_storage_path\": null, \n",
            "    \"persistent_time_interval\": 100, \n",
            "    \"num_of_version_in_retention\": 2, \n",
            "    \"enable_nebula_load\": true, \n",
            "    \"load_path\": null\n",
            "}\n",
            "[2025-05-20 07:10:29,434] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False\n",
            "[2025-05-20 07:10:29,434] [INFO] [config.py:964:print]   optimizer_name ............... None\n",
            "[2025-05-20 07:10:29,434] [INFO] [config.py:964:print]   optimizer_params ............. None\n",
            "[2025-05-20 07:10:29,434] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}\n",
            "[2025-05-20 07:10:29,434] [INFO] [config.py:964:print]   pld_enabled .................. False\n",
            "[2025-05-20 07:10:29,434] [INFO] [config.py:964:print]   pld_params ................... False\n",
            "[2025-05-20 07:10:29,434] [INFO] [config.py:964:print]   prescale_gradients ........... False\n",
            "[2025-05-20 07:10:29,434] [INFO] [config.py:964:print]   scheduler_name ............... None\n",
            "[2025-05-20 07:10:29,434] [INFO] [config.py:964:print]   scheduler_params ............. None\n",
            "[2025-05-20 07:10:29,435] [INFO] [config.py:964:print]   sparse_attention ............. None\n",
            "[2025-05-20 07:10:29,435] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False\n",
            "[2025-05-20 07:10:29,435] [INFO] [config.py:964:print]   steps_per_print .............. inf\n",
            "[2025-05-20 07:10:29,435] [INFO] [config.py:964:print]   train_batch_size ............. 8\n",
            "[2025-05-20 07:10:29,435] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  2\n",
            "[2025-05-20 07:10:29,435] [INFO] [config.py:964:print]   use_node_local_storage ....... False\n",
            "[2025-05-20 07:10:29,435] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False\n",
            "[2025-05-20 07:10:29,435] [INFO] [config.py:964:print]   world_size ................... 1\n",
            "[2025-05-20 07:10:29,435] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  False\n",
            "[2025-05-20 07:10:29,435] [INFO] [config.py:964:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='none', nvme_path=None, buffer_count=5, buffer_size=100,000,000, max_in_cpu=1,000,000,000, pin_memory=True) offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='none', nvme_path=None, buffer_count=4, pin_memory=True, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False) sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=True stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True\n",
            "[2025-05-20 07:10:29,435] [INFO] [config.py:964:print]   zero_enabled ................. False\n",
            "[2025-05-20 07:10:29,435] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True\n",
            "[2025-05-20 07:10:29,435] [INFO] [config.py:964:print]   zero_optimization_stage ...... 0\n",
            "[2025-05-20 07:10:29,435] [INFO] [config.py:950:print_user_config]   json = {\n",
            "    \"zero_optimization\": {\n",
            "        \"stage\": 0, \n",
            "        \"offload_optimizer\": {\n",
            "            \"device\": \"none\", \n",
            "            \"pin_memory\": true\n",
            "        }, \n",
            "        \"offload_param\": {\n",
            "            \"device\": \"none\", \n",
            "            \"pin_memory\": true\n",
            "        }, \n",
            "        \"overlap_comm\": true, \n",
            "        \"contiguous_gradients\": true, \n",
            "        \"sub_group_size\": 1.000000e+09, \n",
            "        \"reduce_bucket_size\": \"auto\", \n",
            "        \"stage3_prefetch_bucket_size\": \"auto\", \n",
            "        \"stage3_param_persistence_threshold\": \"auto\", \n",
            "        \"stage3_max_live_parameters\": 1.000000e+09, \n",
            "        \"stage3_max_reuse_distance\": 1.000000e+09, \n",
            "        \"stage3_gather_16bit_weights_on_model_save\": true\n",
            "    }, \n",
            "    \"train_batch_size\": 8, \n",
            "    \"train_micro_batch_size_per_gpu\": 2, \n",
            "    \"gradient_accumulation_steps\": 4, \n",
            "    \"bf16\": {\n",
            "        \"enabled\": true\n",
            "    }, \n",
            "    \"steps_per_print\": inf, \n",
            "    \"fp16\": {\n",
            "        \"enabled\": false\n",
            "    }, \n",
            "    \"optimizer\": {\n",
            "        \"type\": null\n",
            "    }\n",
            "}\n",
            "  0% 0/31 [00:00<?, ?it/s]Could not estimate the number of tokens of the input, floating-point operations will not be computed\n",
            "{'loss': -0.2221, 'grad_norm': 7.40625, 'learning_rate': 3.3333333333333333e-06, 'epoch': 0.16}\n",
            "{'loss': -0.1768, 'grad_norm': 7.34375, 'learning_rate': 6.666666666666667e-06, 'epoch': 0.32}\n",
            "{'loss': -0.445, 'grad_norm': 9.125, 'learning_rate': 1e-05, 'epoch': 0.48}\n",
            "{'loss': -0.5362, 'grad_norm': 11.3125, 'learning_rate': 1.3333333333333333e-05, 'epoch': 0.64}\n",
            "{'loss': -0.6005, 'grad_norm': 11.4375, 'learning_rate': 1.6666666666666667e-05, 'epoch': 0.8}\n",
            "{'loss': -0.5747, 'grad_norm': 8.5625, 'learning_rate': 2e-05, 'epoch': 0.96}\n",
            "{'loss': -0.351, 'grad_norm': 9.1875, 'learning_rate': 1.9200000000000003e-05, 'epoch': 1.12}\n",
            "{'loss': -0.5979, 'grad_norm': 10.375, 'learning_rate': 1.8400000000000003e-05, 'epoch': 1.28}\n",
            "{'loss': -1.012, 'grad_norm': 12.375, 'learning_rate': 1.76e-05, 'epoch': 1.44}\n",
            "{'loss': -1.0733, 'grad_norm': 16.0, 'learning_rate': 1.6800000000000002e-05, 'epoch': 1.6}\n",
            "{'loss': -0.5926, 'grad_norm': 13.25, 'learning_rate': 1.6000000000000003e-05, 'epoch': 1.76}\n",
            "{'loss': -1.271, 'grad_norm': 15.5625, 'learning_rate': 1.5200000000000002e-05, 'epoch': 1.92}\n",
            "{'loss': -0.7415, 'grad_norm': 16.125, 'learning_rate': 1.4400000000000001e-05, 'epoch': 2.08}\n",
            "{'loss': -1.6378, 'grad_norm': 20.875, 'learning_rate': 1.3600000000000002e-05, 'epoch': 2.24}\n",
            "{'loss': -1.2658, 'grad_norm': 16.375, 'learning_rate': 1.2800000000000001e-05, 'epoch': 2.4}\n",
            "{'loss': -1.4469, 'grad_norm': 22.125, 'learning_rate': 1.2e-05, 'epoch': 2.56}\n",
            "{'loss': -1.7464, 'grad_norm': 26.75, 'learning_rate': 1.1200000000000001e-05, 'epoch': 2.72}\n",
            "{'loss': -2.037, 'grad_norm': 22.125, 'learning_rate': 1.04e-05, 'epoch': 2.88}\n",
            "{'loss': -1.9946, 'grad_norm': 34.0, 'learning_rate': 9.600000000000001e-06, 'epoch': 3.04}\n",
            "{'loss': -2.6231, 'grad_norm': 31.875, 'learning_rate': 8.8e-06, 'epoch': 3.2}\n",
            "{'loss': -2.2177, 'grad_norm': 25.5, 'learning_rate': 8.000000000000001e-06, 'epoch': 3.36}\n",
            "{'loss': -2.8008, 'grad_norm': 30.125, 'learning_rate': 7.2000000000000005e-06, 'epoch': 3.52}\n",
            "{'loss': -1.3649, 'grad_norm': 27.0, 'learning_rate': 6.4000000000000006e-06, 'epoch': 3.68}\n",
            "{'loss': -2.0433, 'grad_norm': 24.625, 'learning_rate': 5.600000000000001e-06, 'epoch': 3.84}\n",
            "{'loss': -2.2082, 'grad_norm': 28.125, 'learning_rate': 4.800000000000001e-06, 'epoch': 4.0}\n",
            "{'loss': -1.8114, 'grad_norm': 23.5, 'learning_rate': 4.000000000000001e-06, 'epoch': 4.16}\n",
            "{'loss': -2.8249, 'grad_norm': 28.625, 'learning_rate': 3.2000000000000003e-06, 'epoch': 4.32}\n",
            "{'loss': -1.8268, 'grad_norm': 25.5, 'learning_rate': 2.4000000000000003e-06, 'epoch': 4.48}\n",
            "{'loss': -2.8893, 'grad_norm': 28.5, 'learning_rate': 1.6000000000000001e-06, 'epoch': 4.64}\n",
            "{'loss': -2.1067, 'grad_norm': 31.125, 'learning_rate': 8.000000000000001e-07, 'epoch': 4.8}\n",
            "{'loss': -3.4172, 'grad_norm': 35.5, 'learning_rate': 0.0, 'epoch': 4.96}\n",
            "{'train_runtime': 98.5465, 'train_samples_per_second': 2.517, 'train_steps_per_second': 0.315, 'train_loss': -1.4986213777334458, 'epoch': 4.96}\n",
            "100% 31/31 [01:38<00:00,  3.18s/it]\n",
            "Forget step completed successfully.\n",
            "===== Step 2: Running evaluation =====\n",
            "Running evaluate_Harry.py...\n",
            "2025-05-20 07:12:26.466166: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1747725146.488945   15460 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1747725146.495661   15460 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "Loading checkpoint from /content/Unlearn_Harry/Harry/experiment/Harry/phi/grad_ascent+kl_E5_B2_G4_lr2e-5_W1\n",
            "Working on eval task eval_log_forget with split forget\n",
            "load data from  data/Harry/forget.json\n",
            "load data from  data/Harry/forget.json\n",
            "load data from  data/Harry/forget.json\n",
            "100% 13/13 [00:02<00:00,  4.59it/s]\n",
            "[2025-05-20 07:12:37,008][absl][INFO] - Using default tokenizer.\n",
            "fluency 2.0891364650449926\n",
            "Working on eval task eval_log_forget_rephrase with split forget\n",
            "load data from  data/Harry/forget.json\n",
            "load data from  data/Harry/forget.json\n",
            "load data from  data/Harry/forget.json\n",
            "100% 13/13 [00:02<00:00,  6.21it/s]\n",
            "[2025-05-20 07:12:39,143][absl][INFO] - Using default tokenizer.\n",
            "fluency 2.106706417152303\n",
            "Working on eval task eval_log_retain with split test_retain_harry\n",
            "load data from  data/test/test_retain_harry.json\n",
            "load data from  data/test/test_retain_harry.json\n",
            "load data from  data/test/test_retain_harry.json\n",
            "100% 38/38 [00:06<00:00,  6.11it/s]\n",
            "[2025-05-20 07:12:45,402][absl][INFO] - Using default tokenizer.\n",
            "fluency 1.6210837074946232\n",
            "Working on eval task eval_log_retain_rephrase with split test_retain_harry\n",
            "load data from  data/test/test_retain_harry.json\n",
            "load data from  data/test/test_retain_harry.json\n",
            "load data from  data/test/test_retain_harry.json\n",
            "100% 38/38 [00:06<00:00,  6.11it/s]\n",
            "[2025-05-20 07:12:51,704][absl][INFO] - Using default tokenizer.\n",
            "fluency 1.6755334909536428\n",
            "Working on eval task eval_real_author_wo_options with split real_authors_perturbed\n",
            "load data from  data/test/real_authors_perturbed.json\n",
            "load data from  data/test/real_authors_perturbed.json\n",
            "load data from  data/test/real_authors_perturbed.json\n",
            "100% 25/25 [02:33<00:00,  6.16s/it]\n",
            "[2025-05-20 07:15:25,688][absl][INFO] - Using default tokenizer.\n",
            "fluency 7.313627044563605\n",
            "100it [00:25,  3.92it/s]\n",
            "Working on eval task eval_real_world_wo_options with split world_facts_perturbed\n",
            "load data from  data/test/world_facts_perturbed.json\n",
            "load data from  data/test/world_facts_perturbed.json\n",
            "load data from  data/test/world_facts_perturbed.json\n",
            "100% 30/30 [03:07<00:00,  6.26s/it]\n",
            "[2025-05-20 07:18:59,471][absl][INFO] - Using default tokenizer.\n",
            "fluency 7.312699291366842\n",
            "117it [00:29,  3.92it/s]\n",
            "Evaluation completed successfully.\n",
            "===== Step 3: Aggregating statistics =====\n",
            "Running aggregate_eval_stat.py...\n",
            "[2025-05-20 07:19:32,911][numexpr.utils][INFO] - NumExpr defaulting to 12 threads.\n",
            "===== All steps completed successfully! =====\n",
            "Results saved to: /content/Unlearn_Harry/Harry/experiment/Harry/phi/grad_ascent+kl_E5_B2_G4_lr2e-5_W1/eval_results/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile scripts/finetune.sh\n",
        "\n",
        "#!/bin/bash\n",
        "export dataset=\"Harry\";   # [TOFU, Harry, ZSRE]\n",
        "export master_port=18765;\n",
        "export model=deepseek-r1-distill;   # [phi, llama2-7b]\n",
        "export split=finetune;\n",
        "export data_path=$PWD/data/${dataset}/${split}.json;\n",
        "export lr=3e-5;\n",
        "export batch_size=1;\n",
        "export GA=2;\n",
        "export epoch=5;\n",
        "export save_file=$PWD/save_model/${dataset}/${split}_${model}_B${batch_size}_G${GA}_E${epoch}_lr${lr};\n",
        "export CUDA_VISIBLE_DEVICES=0;\n",
        "\n",
        "deepspeed finetune.py --config-name finetune.yaml \\\n",
        "    dataset=${dataset} \\\n",
        "    data_path=${data_path} \\\n",
        "    split=${split} \\\n",
        "    batch_size=${batch_size} \\\n",
        "    gradient_accumulation_steps=${GA} \\\n",
        "    num_epochs=${epoch} \\\n",
        "    lr=${lr} \\\n",
        "    model_family=${model} \\\n",
        "    save_dir=${save_file} \\\n",
        "    weight_decay=0.01 \\\n",
        "    bf16=true\n",
        "\n",
        "if [[ ${dataset} = \"TOFU\" ]];\n",
        "then\n",
        "declare -A retain_splits;\n",
        "retain_splits[\"retain99\"]=\"forget01\";\n",
        "retain_splits[\"retain95\"]=\"forget05\";\n",
        "retain_splits[\"retain90\"]=\"forget10\";\n",
        "\n",
        "python evaluate_${dataset}.py \\\n",
        "    model_family=$model dataset=${dataset} \\\n",
        "    split=${retain_splits[${split}]} \\\n",
        "    model_path=$save_file batch_size=${batch_size} \\\n",
        "    generation.max_length=200 \\\n",
        "    save_dir=$save_file/eval_results;\n",
        "\n",
        "python aggregate_eval_stat.py \\\n",
        "    ckpt_result=$save_file/eval_results/eval_log_aggregated.json \\\n",
        "    method_name=\"finetune\" \\\n",
        "    save_file=$save_file/eval_results/eval.csv \\\n",
        "    excel_file_path=$save_file/eval_results/eval.xlsx \\\n",
        "    submitted_by=who;\n",
        "\n",
        "elif [[ ${dataset} = \"Harry\" ]];\n",
        "then\n",
        "python evaluate_${dataset}.py \\\n",
        "    model_family=$model dataset=${dataset} \\\n",
        "    split=forget_all_subject batch_size=${batch_size}\\\n",
        "    model_path=$save_file \\\n",
        "    generation.max_length=200 \\\n",
        "    save_dir=$save_file/eval_results;\n",
        "\n",
        "python aggregate_eval_stat.py \\\n",
        "    ckpt_result=$save_file/eval_results/eval_log_aggregated.json \\\n",
        "    method_name=\"finetune\" \\\n",
        "    save_file=$save_file/eval_results/eval.csv \\\n",
        "    excel_file_path=$save_file/eval_results/eval.xlsx \\\n",
        "    submitted_by=who;\n",
        "\n",
        "elif [[ ${dataset} = \"ZSRE\" ]];\n",
        "then\n",
        "export Types=(inverse onehop subject_replace);\n",
        "for type in \"${Types[@]}\"\n",
        "do\n",
        "    python evaluate_${dataset}.py --config-name eval_${type}.yaml \\\n",
        "        model_family=$model dataset=${dataset} \\\n",
        "        split=forget batch_size=${batch_size} \\\n",
        "        model_path=$save_file \\\n",
        "        generation.max_length=200 \\\n",
        "        save_dir=$save_file/eval_${type}_results; \\\n",
        "\n",
        "    python aggregate_eval_stat.py \\\n",
        "        ckpt_result=$save_file/eval_${type}_results/eval_log_aggregated.json \\\n",
        "        method_name=\"finetune\" \\\n",
        "        save_file=$save_file/eval_${type}_results/eval.csv \\\n",
        "        excel_file_path=$save_file/eval_${type}_results/eval.xlsx \\\n",
        "        submitted_by=who;\n",
        "done\n",
        "fi\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pzHYu21QI_uv",
        "outputId": "691eea0a-766b-4502-b9e4-32f5216b73cb"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting scripts/finetune.sh\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!chmod +x scripts/finetune.sh"
      ],
      "metadata": {
        "id": "_FkApj8wJKhZ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y bitsandbytes\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KbxmoIsrR-Ug",
        "outputId": "9ca72703-57f3-4dfa-cace-29f5703bbd0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: bitsandbytes 0.43.1\n",
            "Uninstalling bitsandbytes-0.43.1:\n",
            "  Successfully uninstalled bitsandbytes-0.43.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y bitsandbytes\n",
        "!pip install bitsandbytes-cuda117  # or cuda118 based on your runtime\n",
        "!pip install accelerate --upgrade\n",
        "!pip install deepspeed --upgrade\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "VuSZs1o9WJmt",
        "outputId": "a71dfee1-7d37-45d7-b546-aa63546323c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Skipping bitsandbytes as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting bitsandbytes-cuda117\n",
            "  Downloading bitsandbytes_cuda117-0.26.0.post2-py3-none-any.whl.metadata (6.3 kB)\n",
            "Downloading bitsandbytes_cuda117-0.26.0.post2-py3-none-any.whl (4.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.3/4.3 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: bitsandbytes-cuda117\n",
            "Successfully installed bitsandbytes-cuda117-0.26.0.post2\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (0.29.2)\n",
            "Collecting accelerate\n",
            "  Downloading accelerate-1.7.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.11/dist-packages (from accelerate) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (24.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.8)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.0.1)\n",
            "Collecting huggingface-hub>=0.21.0 (from accelerate)\n",
            "  Downloading huggingface_hub-0.31.4-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate) (0.4.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (3.13.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (2024.2.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (4.66.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.3)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.7.101)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (8.5.0.96)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.10.3.66)\n",
            "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (10.9.0.58)\n",
            "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (10.2.10.91)\n",
            "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.4.0.1)\n",
            "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.7.4.91)\n",
            "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (2.14.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.7.91)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (2.0.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=2.0.0->accelerate) (75.2.0)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.11/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=2.0.0->accelerate) (0.45.1)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.11/dist-packages (from triton==2.0.0->torch>=2.0.0->accelerate) (3.29.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.11/dist-packages (from triton==2.0.0->torch>=2.0.0->accelerate) (18.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.2.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.11/dist-packages (from sympy->torch>=2.0.0->accelerate) (1.3.0)\n",
            "Downloading accelerate-1.7.0-py3-none-any.whl (362 kB)\n",
            "Downloading huggingface_hub-0.31.4-py3-none-any.whl (489 kB)\n",
            "Installing collected packages: huggingface-hub, accelerate\n",
            "\u001b[2K  Attempting uninstall: huggingface-hub\n",
            "\u001b[2K    Found existing installation: huggingface-hub 0.20.3\n",
            "\u001b[2K    Uninstalling huggingface-hub-0.20.3:\n",
            "\u001b[2K      Successfully uninstalled huggingface-hub-0.20.3\n",
            "\u001b[2K  Attempting uninstall: accelerate\n",
            "\u001b[2K    Found existing installation: accelerate 0.29.2\n",
            "\u001b[2K    Uninstalling accelerate-0.29.2:\n",
            "\u001b[2K      Successfully uninstalled accelerate-0.29.2\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/2\u001b[0m [accelerate]\n",
            "\u001b[1A\u001b[2KSuccessfully installed accelerate-1.7.0 huggingface-hub-0.31.4\n",
            "Requirement already satisfied: deepspeed in /usr/local/lib/python3.11/dist-packages (0.10.0)\n",
            "Collecting deepspeed\n",
            "  Downloading deepspeed-0.16.8.tar.gz (1.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m40.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (from deepspeed) (0.8.1)\n",
            "Requirement already satisfied: hjson in /usr/local/lib/python3.11/dist-packages (from deepspeed) (3.1.0)\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.11/dist-packages (from deepspeed) (1.1.0)\n",
            "Requirement already satisfied: ninja in /usr/local/lib/python3.11/dist-packages (from deepspeed) (1.11.1.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from deepspeed) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from deepspeed) (24.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from deepspeed) (5.9.8)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.11/dist-packages (from deepspeed) (9.0.0)\n",
            "Collecting pydantic>=2.0.0 (from deepspeed)\n",
            "  Downloading pydantic-2.11.4-py3-none-any.whl.metadata (66 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from deepspeed) (2.0.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from deepspeed) (4.66.2)\n",
            "Requirement already satisfied: nvidia-ml-py in /usr/local/lib/python3.11/dist-packages (from deepspeed) (12.575.51)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0.0->deepspeed) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0.0->deepspeed) (2.33.2)\n",
            "Collecting typing-extensions>=4.12.2 (from pydantic>=2.0.0->deepspeed)\n",
            "  Downloading typing_extensions-4.13.2-py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0.0->deepspeed) (0.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (3.13.4)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (3.1.3)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (11.7.101)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (8.5.0.96)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (11.10.3.66)\n",
            "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (10.9.0.58)\n",
            "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (10.2.10.91)\n",
            "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (11.4.0.1)\n",
            "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (11.7.4.91)\n",
            "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (2.14.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (11.7.91)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (2.0.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch->deepspeed) (75.2.0)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.11/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch->deepspeed) (0.45.1)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.11/dist-packages (from triton==2.0.0->torch->deepspeed) (3.29.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.11/dist-packages (from triton==2.0.0->torch->deepspeed) (18.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->deepspeed) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.11/dist-packages (from sympy->torch->deepspeed) (1.3.0)\n",
            "Downloading pydantic-2.11.4-py3-none-any.whl (443 kB)\n",
            "Downloading typing_extensions-4.13.2-py3-none-any.whl (45 kB)\n",
            "Building wheels for collected packages: deepspeed\n",
            "\u001b[33m  DEPRECATION: Building 'deepspeed' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'deepspeed'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\n",
            "\u001b[0m  Building wheel for deepspeed (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for deepspeed: filename=deepspeed-0.16.8-py3-none-any.whl size=1642762 sha256=c3defb7362637801dbb1dae89ed4e47e23ea9799d989baa552069ecaad728e4d\n",
            "  Stored in directory: /root/.cache/pip/wheels/66/4e/6a/91f4af9915a2d18a7d01f29072331228d9a5b1a1ea6722972a\n",
            "Successfully built deepspeed\n",
            "Installing collected packages: typing-extensions, pydantic, deepspeed\n",
            "\u001b[2K  Attempting uninstall: typing-extensions\n",
            "\u001b[2K    Found existing installation: typing_extensions 4.11.0\n",
            "\u001b[2K    Uninstalling typing_extensions-4.11.0:\n",
            "\u001b[2K      Successfully uninstalled typing_extensions-4.11.0\n",
            "\u001b[2K  Attempting uninstall: pydantic\n",
            "\u001b[2K    Found existing installation: pydantic 1.10.15\n",
            "\u001b[2K    Uninstalling pydantic-1.10.15:\n",
            "\u001b[2K      Successfully uninstalled pydantic-1.10.15\n",
            "\u001b[2K  Attempting uninstall: deepspeed\n",
            "\u001b[2K    Found existing installation: deepspeed 0.10.0\n",
            "\u001b[2K    Uninstalling deepspeed-0.10.0:\n",
            "\u001b[2K      Successfully uninstalled deepspeed-0.10.0\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/3\u001b[0m [deepspeed]\n",
            "\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "orbax-checkpoint 0.11.13 requires jax>=0.5.0, which is not installed.\n",
            "flax 0.10.6 requires jax>=0.5.1, which is not installed.\n",
            "dopamine-rl 4.1.2 requires jax>=0.1.72, which is not installed.\n",
            "dopamine-rl 4.1.2 requires jaxlib>=0.1.51, which is not installed.\n",
            "chex 0.1.89 requires jax>=0.4.27, which is not installed.\n",
            "chex 0.1.89 requires jaxlib>=0.4.27, which is not installed.\n",
            "optax 0.2.4 requires jax>=0.4.27, which is not installed.\n",
            "optax 0.2.4 requires jaxlib>=0.4.27, which is not installed.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.23.5 which is incompatible.\n",
            "pymc 5.22.0 requires numpy>=1.25.0, but you have numpy 1.23.5 which is incompatible.\n",
            "chex 0.1.89 requires numpy>=1.24.1, but you have numpy 1.23.5 which is incompatible.\n",
            "albumentations 2.0.6 requires numpy>=1.24.4, but you have numpy 1.23.5 which is incompatible.\n",
            "bigframes 2.4.0 requires numpy>=1.24.0, but you have numpy 1.23.5 which is incompatible.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 1.23.5 which is incompatible.\n",
            "tensorflow 2.18.0 requires tensorboard<2.19,>=2.18, but you have tensorboard 2.14.0 which is incompatible.\n",
            "torchaudio 2.6.0+cu124 requires torch==2.6.0, but you have torch 2.0.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed deepspeed-0.16.8 pydantic-2.11.4 typing-extensions-4.13.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:128\"\n"
      ],
      "metadata": {
        "id": "z1-7j4GJz8l9"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "qg5_OzT60dht",
        "outputId": "53081753-5382-4393-92a8-651f8d423ae5"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue May 20 08:31:47 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  NVIDIA A100-SXM4-40GB          Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   33C    P0             46W /  400W |       0MiB /  40960MiB |      0%      Default |\n",
            "|                                         |                        |             Disabled |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!bash scripts/finetune.sh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "TYZ1FGW0s2sG",
        "outputId": "620a9bc9-36da-445c-87f1-38eb6c66e695"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-05-21 06:10:52,633] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
            "[2025-05-21 06:10:55,029] [WARNING] [runner.py:196:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.\n",
            "Detected CUDA_VISIBLE_DEVICES=0: setting --include=localhost:0\n",
            "[2025-05-21 06:10:55,046] [INFO] [runner.py:555:main] cmd = /usr/bin/python3 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None finetune.py --config-name finetune.yaml dataset=Harry data_path=/content/Unlearn_Harry/Harry/data/Harry/finetune.json split=finetune batch_size=1 gradient_accumulation_steps=2 num_epochs=3 lr=3e-5 model_family=deepseek-r1-distill save_dir=/content/Unlearn_Harry/Harry/save_model/Harry/finetune_deepseek-r1-distill_B1_G2_E3_lr3e-5 weight_decay=0.01 bf16=true\n",
            "[2025-05-21 06:10:57,098] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
            "[2025-05-21 06:10:58,790] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_DEV_PACKAGE=libnccl-dev=2.22.3-1+cuda12.5\n",
            "[2025-05-21 06:10:58,790] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_DEV_PACKAGE_VERSION=2.22.3-1\n",
            "[2025-05-21 06:10:58,790] [INFO] [launch.py:138:main] 0 NCCL_VERSION=2.22.3-1\n",
            "[2025-05-21 06:10:58,790] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_DEV_PACKAGE_NAME=libnccl-dev\n",
            "[2025-05-21 06:10:58,791] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_PACKAGE=libnccl2=2.22.3-1+cuda12.5\n",
            "[2025-05-21 06:10:58,791] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_PACKAGE_NAME=libnccl2\n",
            "[2025-05-21 06:10:58,791] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_PACKAGE_VERSION=2.22.3-1\n",
            "[2025-05-21 06:10:58,791] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0]}\n",
            "[2025-05-21 06:10:58,791] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0\n",
            "[2025-05-21 06:10:58,791] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})\n",
            "[2025-05-21 06:10:58,791] [INFO] [launch.py:163:main] dist_world_size=1\n",
            "[2025-05-21 06:10:58,791] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0\n",
            "2025-05-21 06:11:02.268511: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1747807862.291207   10421 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1747807862.298101   10421 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "[2025-05-21 06:11:05,807] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "tokenizer_config.json: 100% 3.07k/3.07k [00:00<00:00, 23.6MB/s]\n",
            "tokenizer.json: 100% 7.03M/7.03M [00:00<00:00, 23.1MB/s]\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "load data from  /content/Unlearn_Harry/Harry/data/Harry/finetune.json\n",
            "num_devices: 1\n",
            "max_steps: 300\n",
            "[2025-05-21 06:11:08,708] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
            "[2025-05-21 06:11:08,708] [INFO] [comm.py:616:init_distributed] cdb=None\n",
            "[2025-05-21 06:11:08,708] [INFO] [comm.py:643:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
            "[2025-05-21 06:11:08,709][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0\n",
            "[2025-05-21 06:11:08,709][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.\n",
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "config.json: 100% 679/679 [00:00<00:00, 5.85MB/s]\n",
            "model.safetensors: 100% 3.55G/3.55G [00:13<00:00, 254MB/s]\n",
            "generation_config.json: 100% 181/181 [00:00<00:00, 1.67MB/s]\n",
            "max_steps is given, it will override any value given in num_train_epochs\n",
            "[2025-05-21 06:11:29,376][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:2 to store for rank: 0\n",
            "[2025-05-21 06:11:29,376][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:2 with 1 nodes.\n",
            "  0% 0/300 [00:00<?, ?it/s]Could not estimate the number of tokens of the input, floating-point operations will not be computed\n",
            "{'loss': 3.574, 'grad_norm': 187.10331996987654, 'learning_rate': 4.5e-06, 'epoch': 0.15}\n",
            "{'loss': 2.0265, 'grad_norm': 41.959316349478634, 'learning_rate': 9e-06, 'epoch': 0.3}\n",
            "{'loss': 1.5927, 'grad_norm': 23.39586666803428, 'learning_rate': 1.3500000000000001e-05, 'epoch': 0.45}\n",
            "{'loss': 1.431, 'grad_norm': 35.993511038992324, 'learning_rate': 1.8e-05, 'epoch': 0.6}\n",
            "{'loss': 1.3513, 'grad_norm': 28.111164131499873, 'learning_rate': 2.25e-05, 'epoch': 0.75}\n",
            "{'loss': 1.1681, 'grad_norm': 23.06875682614705, 'learning_rate': 2.7000000000000002e-05, 'epoch': 0.9}\n",
            "{'loss': 1.1454, 'grad_norm': 9.075302026520031, 'learning_rate': 2.925e-05, 'epoch': 1.05}\n",
            "{'loss': 0.5101, 'grad_norm': 9.534678958635066, 'learning_rate': 2.7000000000000002e-05, 'epoch': 1.2}\n",
            "{'loss': 0.6967, 'grad_norm': 25.077336436200216, 'learning_rate': 2.475e-05, 'epoch': 1.35}\n",
            "{'loss': 0.6166, 'grad_norm': 13.944296509733547, 'learning_rate': 2.25e-05, 'epoch': 1.5}\n",
            "{'loss': 0.5632, 'grad_norm': 21.59756899681688, 'learning_rate': 2.025e-05, 'epoch': 1.65}\n",
            "{'loss': 0.6378, 'grad_norm': 9.542152048172845, 'learning_rate': 1.8e-05, 'epoch': 1.8}\n",
            "{'loss': 0.7576, 'grad_norm': 8.14063718359213, 'learning_rate': 1.575e-05, 'epoch': 1.95}\n",
            "{'loss': 0.2826, 'grad_norm': 8.305804086947978, 'learning_rate': 1.3500000000000001e-05, 'epoch': 2.1}\n",
            "{'loss': 0.1587, 'grad_norm': 7.513257833584961, 'learning_rate': 1.125e-05, 'epoch': 2.25}\n",
            "{'loss': 0.2221, 'grad_norm': 13.565060422195417, 'learning_rate': 9e-06, 'epoch': 2.4}\n",
            "{'loss': 0.1174, 'grad_norm': 19.378112936669766, 'learning_rate': 6.750000000000001e-06, 'epoch': 2.55}\n",
            "{'loss': 0.1626, 'grad_norm': 6.962825977084176, 'learning_rate': 4.5e-06, 'epoch': 2.7}\n",
            "{'loss': 0.1009, 'grad_norm': 7.851236972418443, 'learning_rate': 2.25e-06, 'epoch': 2.85}\n",
            "{'loss': 0.1398, 'grad_norm': 6.770351952288841, 'learning_rate': 0.0, 'epoch': 3.0}\n",
            "{'train_runtime': 2261.2162, 'train_samples_per_second': 0.265, 'train_steps_per_second': 0.133, 'train_loss': 0.8627578091621398, 'epoch': 3.0}\n",
            "100% 300/300 [37:41<00:00,  7.54s/it]\n",
            "[2025-05-21 06:49:27,074] [INFO] [launch.py:347:main] Process 10421 exits successfully.\n",
            "2025-05-21 06:49:33.017306: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1747810173.039000   20396 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1747810173.045561   20396 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "Loading checkpoint from /content/Unlearn_Harry/Harry/save_model/Harry/finetune_deepseek-r1-distill_B1_G2_E3_lr3e-5\n",
            "Working on eval task eval_log_forget with split forget_all_subject\n",
            "load data from  data/Harry/forget_all_subject.json\n",
            "Error executing job with overrides: ['model_family=deepseek-r1-distill', 'dataset=Harry', 'split=forget_all_subject', 'batch_size=1', 'model_path=/content/Unlearn_Harry/Harry/save_model/Harry/finetune_deepseek-r1-distill_B1_G2_E3_lr3e-5', 'generation.max_length=200', 'save_dir=/content/Unlearn_Harry/Harry/save_model/Harry/finetune_deepseek-r1-distill_B1_G2_E3_lr3e-5/eval_results']\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/Unlearn_Harry/Harry/evaluate_Harry.py\", line 465, in main\n",
            "    eval_dataloader, base_eval_dataloader, perturb_dataloader = get_dataloader(cfg, cfg.forget_loss, tokenizer, folder, split, question_key, answer_key, base_answer_key, perturbed_answer_key)\n",
            "                                                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Unlearn_Harry/Harry/evaluate_Harry.py\", line 94, in get_dataloader\n",
            "    torch_format_dataset = CommonDataset(\n",
            "                          ^^^^^^^^^^^^^^\n",
            "  File \"/content/Unlearn_Harry/Harry/data_module.py\", line 209, in __init__\n",
            "    with open(data_path, 'r') as json_file:\n",
            "         ^^^^^^^^^^^^^^^^^^^^\n",
            "FileNotFoundError: [Errno 2] No such file or directory: 'data/Harry/forget_all_subject.json'\n",
            "\n",
            "Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.\n",
            "Error executing job with overrides: ['ckpt_result=/content/Unlearn_Harry/Harry/save_model/Harry/finetune_deepseek-r1-distill_B1_G2_E3_lr3e-5/eval_results/eval_log_aggregated.json', 'method_name=finetune', 'save_file=/content/Unlearn_Harry/Harry/save_model/Harry/finetune_deepseek-r1-distill_B1_G2_E3_lr3e-5/eval_results/eval.csv', 'excel_file_path=/content/Unlearn_Harry/Harry/save_model/Harry/finetune_deepseek-r1-distill_B1_G2_E3_lr3e-5/eval_results/eval.xlsx', 'submitted_by=who']\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/Unlearn_Harry/Harry/aggregate_eval_stat.py\", line 90, in main\n",
            "    ckpt_result = json.load(open(cfg.ckpt_result))\n",
            "                            ^^^^^^^^^^^^^^^^^^^^^\n",
            "FileNotFoundError: [Errno 2] No such file or directory: '/content/Unlearn_Harry/Harry/save_model/Harry/finetune_deepseek-r1-distill_B1_G2_E3_lr3e-5/eval_results/eval_log_aggregated.json'\n",
            "\n",
            "Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile scripts/eval.sh\n",
        "\n",
        "#!/bin/bash\n",
        "export dataset=\"Harry\";   # [TOFU, Harry, ZSRE]\n",
        "export master_port=18765;\n",
        "export model=deepseek-r1-distill;   # [phi, llama2-7b]\n",
        "export split=finetune;\n",
        "export data_path=$PWD/data/${dataset}/${split}.json;\n",
        "export lr=3e-5;\n",
        "export batch_size=1;\n",
        "export GA=2;\n",
        "export epoch=3;\n",
        "export save_file=$PWD/save_model/${dataset}/${split}_${model}_B${batch_size}_G${GA}_E${epoch}_lr${lr};\n",
        "export CUDA_VISIBLE_DEVICES=0;\n",
        "\n",
        "if [[ ${dataset} = \"TOFU\" ]];\n",
        "then\n",
        "declare -A retain_splits;\n",
        "retain_splits[\"retain99\"]=\"forget01\";\n",
        "retain_splits[\"retain95\"]=\"forget05\";\n",
        "retain_splits[\"retain90\"]=\"forget10\";\n",
        "\n",
        "python evaluate_${dataset}.py \\\n",
        "    model_family=$model dataset=${dataset} \\\n",
        "    split=${retain_splits[${split}]} \\\n",
        "    model_path=$save_file batch_size=${batch_size} \\\n",
        "    generation.max_length=200 \\\n",
        "    save_dir=$save_file/eval_results;\n",
        "\n",
        "python aggregate_eval_stat.py \\\n",
        "    ckpt_result=$save_file/eval_results/eval_log_aggregated.json \\\n",
        "    method_name=\"finetune\" \\\n",
        "    save_file=$save_file/eval_results/eval.csv \\\n",
        "    excel_file_path=$save_file/eval_results/eval.xlsx \\\n",
        "    submitted_by=SameedandTaha;\n",
        "\n",
        "elif [[ ${dataset} = \"Harry\" ]];\n",
        "then\n",
        "python evaluate_${dataset}.py \\\n",
        "    model_family=$model dataset=${dataset} \\\n",
        "    split=forget batch_size=${batch_size}\\\n",
        "    model_path=$save_file \\\n",
        "    generation.max_length=200 \\\n",
        "    save_dir=$save_file/eval_results;\n",
        "\n",
        "python aggregate_eval_stat.py \\\n",
        "    ckpt_result=$save_file/eval_results/eval_log_aggregated.json \\\n",
        "    method_name=\"finetune\" \\\n",
        "    save_file=$save_file/eval_results/eval.csv \\\n",
        "    excel_file_path=$save_file/eval_results/eval.xlsx \\\n",
        "    submitted_by=SameedandTaha;\n",
        "\n",
        "elif [[ ${dataset} = \"ZSRE\" ]];\n",
        "then\n",
        "export Types=(inverse onehop subject_replace);\n",
        "for type in \"${Types[@]}\"\n",
        "do\n",
        "    python evaluate_${dataset}.py --config-name eval_${type}.yaml \\\n",
        "        model_family=$model dataset=${dataset} \\\n",
        "        split=forget batch_size=${batch_size} \\\n",
        "        model_path=$save_file \\\n",
        "        generation.max_length=200 \\\n",
        "        save_dir=$save_file/eval_${type}_results; \\\n",
        "\n",
        "    python aggregate_eval_stat.py \\\n",
        "        ckpt_result=$save_file/eval_${type}_results/eval_log_aggregated.json \\\n",
        "        method_name=\"finetune\" \\\n",
        "        save_file=$save_file/eval_${type}_results/eval.csv \\\n",
        "        excel_file_path=$save_file/eval_${type}_results/eval.xlsx \\\n",
        "        submitted_by=who;\n",
        "done\n",
        "fi\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iqWPzq6CIdJJ",
        "outputId": "47e93341-6d19-4b62-cffc-6f341d6ca256"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing scripts/eval.sh\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!chmod +x scripts/eval.sh"
      ],
      "metadata": {
        "id": "ErpuyL8xoRX5"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!bash scripts/eval.sh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Hy9iVcYaobAN",
        "outputId": "9105730f-70b2-42b8-9aac-4c9fbb7a367e"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-05-21 08:29:46.882494: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1747816186.904238   47204 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1747816186.910912   47204 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "Loading checkpoint from /content/Unlearn_Harry/Harry/save_model/Harry/finetune_deepseek-r1-distill_B1_G2_E3_lr3e-5/checkpoint-300\n",
            "Working on eval task eval_log_forget with split finetune\n",
            "load data from  data/Harry/finetune.json\n",
            "load data from  data/Harry/finetune.json\n",
            "load data from  data/Harry/finetune.json\n",
            "100% 50/50 [00:16<00:00,  3.04it/s]\n",
            "[2025-05-21 08:30:14,268][absl][INFO] - Using default tokenizer.\n",
            "fluency 3.4648499260926484\n",
            "Working on eval task eval_log_forget_rephrase with split finetune\n",
            "load data from  data/Harry/finetune.json\n",
            "load data from  data/Harry/finetune.json\n",
            "load data from  data/Harry/finetune.json\n",
            "100% 50/50 [00:15<00:00,  3.18it/s]\n",
            "[2025-05-21 08:30:30,144][absl][INFO] - Using default tokenizer.\n",
            "fluency 3.4782181590756225\n",
            "Working on eval task eval_log_retain with split test_retain_harry\n",
            "load data from  data/test/test_retain_harry.json\n",
            "load data from  data/test/test_retain_harry.json\n",
            "load data from  data/test/test_retain_harry.json\n",
            "100% 38/38 [00:11<00:00,  3.18it/s]\n",
            "[2025-05-21 08:30:42,227][absl][INFO] - Using default tokenizer.\n",
            "fluency 3.5047642544032325\n",
            "Working on eval task eval_log_retain_rephrase with split test_retain_harry\n",
            "load data from  data/test/test_retain_harry.json\n",
            "load data from  data/test/test_retain_harry.json\n",
            "load data from  data/test/test_retain_harry.json\n",
            "100% 38/38 [00:11<00:00,  3.18it/s]\n",
            "[2025-05-21 08:30:54,285][absl][INFO] - Using default tokenizer.\n",
            "fluency 3.504543590073086\n",
            "Working on eval task eval_real_author_wo_options with split real_authors_perturbed\n",
            "load data from  data/test/real_authors_perturbed.json\n",
            "load data from  data/test/real_authors_perturbed.json\n",
            "load data from  data/test/real_authors_perturbed.json\n",
            "  0% 0/25 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:497: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
            "  warnings.warn(\n",
            "100% 25/25 [00:15<00:00,  1.58it/s]\n",
            "[2025-05-21 08:31:10,191][absl][INFO] - Using default tokenizer.\n",
            "fluency 0.21416122899716586\n",
            "100it [00:57,  1.74it/s]\n",
            "Working on eval task eval_real_world_wo_options with split world_facts_perturbed\n",
            "load data from  data/test/world_facts_perturbed.json\n",
            "load data from  data/test/world_facts_perturbed.json\n",
            "load data from  data/test/world_facts_perturbed.json\n",
            "100% 30/30 [00:17<00:00,  1.70it/s]\n",
            "[2025-05-21 08:32:25,276][absl][INFO] - Using default tokenizer.\n",
            "fluency 0.2989023906125901\n",
            "117it [01:07,  1.74it/s]\n",
            "[2025-05-21 08:33:35,176][numexpr.utils][INFO] - NumExpr defaulting to 12 threads.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!bash scripts/run_unlearning.sh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "US2cde6nzIcf",
        "outputId": "778cc01a-7539-41f6-bd0e-dabf38e93fc2"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "===== Starting Harry Potter unlearning experiment =====\n",
            "Creating necessary directories...\n",
            "===== Configuration =====\n",
            "Dataset: Harry\n",
            "Model: deepseek-r1-distill\n",
            "Forget method: grad_ascent+kl\n",
            "Save directory: /content/Unlearn_Harry/Harry/experiment/Harry/deepseek-r1-distill/grad_ascent+kl_E1_B2_G4_lr2e-5_W1\n",
            "=======================\n",
            "===== Step 1: Running the forget step =====\n",
            "Running forget.py...\n",
            "2025-05-21 11:03:29.227114: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1747825409.249521   87247 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1747825409.256322   87247 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "[2025-05-21 11:03:32,999] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
            "num_devices: 1\n",
            "######################\n",
            "Saving to:  /content/Unlearn_Harry/Harry/experiment/Harry/deepseek-r1-distill/grad_ascent+kl_E1_B2_G4_lr2e-5_W1\n",
            "######################\n",
            "Directory already exists\n",
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "Loading forget data number 50\n",
            "Loading retain data number 150\n",
            "max_steps: 6\n",
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "Loading from checkpoint\n",
            "max_steps is given, it will override any value given in num_train_epochs\n",
            "[2025-05-21 11:03:52,393] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown\n",
            "[2025-05-21 11:03:52,393] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
            "[2025-05-21 11:03:52,393] [INFO] [comm.py:616:init_distributed] cdb=None\n",
            "[2025-05-21 11:03:52,393] [INFO] [comm.py:627:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...\n",
            "[2025-05-21 11:03:53,329] [INFO] [comm.py:677:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=172.28.0.12, master_port=29500\n",
            "[2025-05-21 11:03:53,329] [INFO] [comm.py:643:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
            "[2025-05-21 11:03:53,331][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0\n",
            "[2025-05-21 11:03:53,331][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.\n",
            "[2025-05-21 11:03:55,591][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:2 to store for rank: 0\n",
            "[2025-05-21 11:03:55,591][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:2 with 1 nodes.\n",
            "[2025-05-21 11:03:55,823] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
            "[2025-05-21 11:03:55,825] [INFO] [logging.py:96:log_dist] [Rank 0] Creating BF16 optimizer\n",
            "[2025-05-21 11:03:56,176] [INFO] [utils.py:785:see_memory_usage] begin bf16_optimizer\n",
            "[2025-05-21 11:03:56,177] [INFO] [utils.py:786:see_memory_usage] MA 5.06 GB         Max_MA 5.06 GB         CA 5.25 GB         Max_CA 5 GB \n",
            "[2025-05-21 11:03:56,177] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 14.61 GB, percent = 17.5%\n",
            "[2025-05-21 11:03:56,490] [INFO] [utils.py:785:see_memory_usage] end bf16_optimizer\n",
            "[2025-05-21 11:03:56,491] [INFO] [utils.py:786:see_memory_usage] MA 5.06 GB         Max_MA 5.06 GB         CA 5.25 GB         Max_CA 5 GB \n",
            "[2025-05-21 11:03:56,491] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 14.61 GB, percent = 17.5%\n",
            "[2025-05-21 11:03:56,492] [INFO] [config.py:960:print] DeepSpeedEngine configuration:\n",
            "[2025-05-21 11:03:56,492] [INFO] [config.py:964:print]   activation_checkpointing_config  {\n",
            "    \"partition_activations\": false, \n",
            "    \"contiguous_memory_optimization\": false, \n",
            "    \"cpu_checkpointing\": false, \n",
            "    \"number_checkpoints\": null, \n",
            "    \"synchronize_checkpoint_boundary\": false, \n",
            "    \"profile\": false\n",
            "}\n",
            "[2025-05-21 11:03:56,492] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
            "[2025-05-21 11:03:56,493] [INFO] [config.py:964:print]   amp_enabled .................. False\n",
            "[2025-05-21 11:03:56,493] [INFO] [config.py:964:print]   amp_params ................... False\n",
            "[2025-05-21 11:03:56,493] [INFO] [config.py:964:print]   autotuning_config ............ {\n",
            "    \"enabled\": false, \n",
            "    \"start_step\": null, \n",
            "    \"end_step\": null, \n",
            "    \"metric_path\": null, \n",
            "    \"arg_mappings\": null, \n",
            "    \"metric\": \"throughput\", \n",
            "    \"model_info\": null, \n",
            "    \"results_dir\": \"autotuning_results\", \n",
            "    \"exps_dir\": \"autotuning_exps\", \n",
            "    \"overwrite\": true, \n",
            "    \"fast\": true, \n",
            "    \"start_profile_step\": 3, \n",
            "    \"end_profile_step\": 5, \n",
            "    \"tuner_type\": \"gridsearch\", \n",
            "    \"tuner_early_stopping\": 5, \n",
            "    \"tuner_num_trials\": 50, \n",
            "    \"model_info_path\": null, \n",
            "    \"mp_size\": 1, \n",
            "    \"max_train_batch_size\": null, \n",
            "    \"min_train_batch_size\": 1, \n",
            "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
            "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
            "    \"num_tuning_micro_batch_sizes\": 3\n",
            "}\n",
            "[2025-05-21 11:03:56,493] [INFO] [config.py:964:print]   bfloat16_enabled ............. True\n",
            "[2025-05-21 11:03:56,493] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False\n",
            "[2025-05-21 11:03:56,493] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True\n",
            "[2025-05-21 11:03:56,493] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False\n",
            "[2025-05-21 11:03:56,493] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x136d65c304d0>\n",
            "[2025-05-21 11:03:56,493] [INFO] [config.py:964:print]   communication_data_type ...... None\n",
            "[2025-05-21 11:03:56,493] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
            "[2025-05-21 11:03:56,493] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False\n",
            "[2025-05-21 11:03:56,493] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False\n",
            "[2025-05-21 11:03:56,493] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
            "[2025-05-21 11:03:56,493] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False\n",
            "[2025-05-21 11:03:56,493] [INFO] [config.py:964:print]   dataloader_drop_last ......... False\n",
            "[2025-05-21 11:03:56,493] [INFO] [config.py:964:print]   disable_allgather ............ False\n",
            "[2025-05-21 11:03:56,493] [INFO] [config.py:964:print]   dump_state ................... False\n",
            "[2025-05-21 11:03:56,493] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... None\n",
            "[2025-05-21 11:03:56,493] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False\n",
            "[2025-05-21 11:03:56,493] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1\n",
            "[2025-05-21 11:03:56,493] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
            "[2025-05-21 11:03:56,493] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0\n",
            "[2025-05-21 11:03:56,493] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100\n",
            "[2025-05-21 11:03:56,493] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06\n",
            "[2025-05-21 11:03:56,493] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01\n",
            "[2025-05-21 11:03:56,494] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False\n",
            "[2025-05-21 11:03:56,494] [INFO] [config.py:964:print]   elasticity_enabled ........... False\n",
            "[2025-05-21 11:03:56,494] [INFO] [config.py:964:print]   flops_profiler_config ........ {\n",
            "    \"enabled\": false, \n",
            "    \"recompute_fwd_factor\": 0.0, \n",
            "    \"profile_step\": 1, \n",
            "    \"module_depth\": -1, \n",
            "    \"top_modules\": 1, \n",
            "    \"detailed\": true, \n",
            "    \"output_file\": null\n",
            "}\n",
            "[2025-05-21 11:03:56,494] [INFO] [config.py:964:print]   fp16_auto_cast ............... None\n",
            "[2025-05-21 11:03:56,494] [INFO] [config.py:964:print]   fp16_enabled ................. False\n",
            "[2025-05-21 11:03:56,494] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False\n",
            "[2025-05-21 11:03:56,494] [INFO] [config.py:964:print]   global_rank .................. 0\n",
            "[2025-05-21 11:03:56,494] [INFO] [config.py:964:print]   grad_accum_dtype ............. None\n",
            "[2025-05-21 11:03:56,494] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 4\n",
            "[2025-05-21 11:03:56,494] [INFO] [config.py:964:print]   gradient_clipping ............ 0.0\n",
            "[2025-05-21 11:03:56,494] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0\n",
            "[2025-05-21 11:03:56,494] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
            "[2025-05-21 11:03:56,494] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 1\n",
            "[2025-05-21 11:03:56,494] [INFO] [config.py:964:print]   load_universal_checkpoint .... False\n",
            "[2025-05-21 11:03:56,494] [INFO] [config.py:964:print]   loss_scale ................... 1.0\n",
            "[2025-05-21 11:03:56,494] [INFO] [config.py:964:print]   memory_breakdown ............. False\n",
            "[2025-05-21 11:03:56,494] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False\n",
            "[2025-05-21 11:03:56,494] [INFO] [config.py:964:print]   mics_shard_size .............. -1\n",
            "[2025-05-21 11:03:56,494] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n",
            "[2025-05-21 11:03:56,494] [INFO] [config.py:964:print]   nebula_config ................ {\n",
            "    \"enabled\": false, \n",
            "    \"persistent_storage_path\": null, \n",
            "    \"persistent_time_interval\": 100, \n",
            "    \"num_of_version_in_retention\": 2, \n",
            "    \"enable_nebula_load\": true, \n",
            "    \"load_path\": null\n",
            "}\n",
            "[2025-05-21 11:03:56,494] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False\n",
            "[2025-05-21 11:03:56,494] [INFO] [config.py:964:print]   optimizer_name ............... None\n",
            "[2025-05-21 11:03:56,494] [INFO] [config.py:964:print]   optimizer_params ............. None\n",
            "[2025-05-21 11:03:56,494] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}\n",
            "[2025-05-21 11:03:56,494] [INFO] [config.py:964:print]   pld_enabled .................. False\n",
            "[2025-05-21 11:03:56,494] [INFO] [config.py:964:print]   pld_params ................... False\n",
            "[2025-05-21 11:03:56,494] [INFO] [config.py:964:print]   prescale_gradients ........... False\n",
            "[2025-05-21 11:03:56,494] [INFO] [config.py:964:print]   scheduler_name ............... None\n",
            "[2025-05-21 11:03:56,494] [INFO] [config.py:964:print]   scheduler_params ............. None\n",
            "[2025-05-21 11:03:56,494] [INFO] [config.py:964:print]   sparse_attention ............. None\n",
            "[2025-05-21 11:03:56,494] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False\n",
            "[2025-05-21 11:03:56,494] [INFO] [config.py:964:print]   steps_per_print .............. inf\n",
            "[2025-05-21 11:03:56,494] [INFO] [config.py:964:print]   train_batch_size ............. 8\n",
            "[2025-05-21 11:03:56,494] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  2\n",
            "[2025-05-21 11:03:56,494] [INFO] [config.py:964:print]   use_node_local_storage ....... False\n",
            "[2025-05-21 11:03:56,494] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False\n",
            "[2025-05-21 11:03:56,494] [INFO] [config.py:964:print]   world_size ................... 1\n",
            "[2025-05-21 11:03:56,494] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  False\n",
            "[2025-05-21 11:03:56,495] [INFO] [config.py:964:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='none', nvme_path=None, buffer_count=5, buffer_size=100,000,000, max_in_cpu=1,000,000,000, pin_memory=True) offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='none', nvme_path=None, buffer_count=4, pin_memory=True, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False) sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=True stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True\n",
            "[2025-05-21 11:03:56,495] [INFO] [config.py:964:print]   zero_enabled ................. False\n",
            "[2025-05-21 11:03:56,495] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True\n",
            "[2025-05-21 11:03:56,495] [INFO] [config.py:964:print]   zero_optimization_stage ...... 0\n",
            "[2025-05-21 11:03:56,495] [INFO] [config.py:950:print_user_config]   json = {\n",
            "    \"zero_optimization\": {\n",
            "        \"stage\": 0, \n",
            "        \"offload_optimizer\": {\n",
            "            \"device\": \"none\", \n",
            "            \"pin_memory\": true\n",
            "        }, \n",
            "        \"offload_param\": {\n",
            "            \"device\": \"none\", \n",
            "            \"pin_memory\": true\n",
            "        }, \n",
            "        \"overlap_comm\": true, \n",
            "        \"contiguous_gradients\": true, \n",
            "        \"sub_group_size\": 1.000000e+09, \n",
            "        \"reduce_bucket_size\": \"auto\", \n",
            "        \"stage3_prefetch_bucket_size\": \"auto\", \n",
            "        \"stage3_param_persistence_threshold\": \"auto\", \n",
            "        \"stage3_max_live_parameters\": 1.000000e+09, \n",
            "        \"stage3_max_reuse_distance\": 1.000000e+09, \n",
            "        \"stage3_gather_16bit_weights_on_model_save\": true\n",
            "    }, \n",
            "    \"train_batch_size\": 8, \n",
            "    \"train_micro_batch_size_per_gpu\": 2, \n",
            "    \"gradient_accumulation_steps\": 4, \n",
            "    \"bf16\": {\n",
            "        \"enabled\": true\n",
            "    }, \n",
            "    \"steps_per_print\": inf, \n",
            "    \"fp16\": {\n",
            "        \"enabled\": false\n",
            "    }, \n",
            "    \"optimizer\": {\n",
            "        \"type\": null\n",
            "    }\n",
            "}\n",
            "  0% 0/6 [00:00<?, ?it/s]Could not estimate the number of tokens of the input, floating-point operations will not be computed\n",
            "{'loss': -0.0278, 'grad_norm': 1.625, 'learning_rate': 3.3333333333333333e-06, 'epoch': 0.16}\n",
            "{'loss': -0.0458, 'grad_norm': 2.359375, 'learning_rate': 6.666666666666667e-06, 'epoch': 0.32}\n",
            "{'loss': -0.039, 'grad_norm': 13.6875, 'learning_rate': 1e-05, 'epoch': 0.48}\n",
            "{'loss': 0.0225, 'grad_norm': 27.0, 'learning_rate': 1.3333333333333333e-05, 'epoch': 0.64}\n",
            "{'loss': 0.0239, 'grad_norm': 24.5, 'learning_rate': 1.6666666666666667e-05, 'epoch': 0.8}\n",
            "{'loss': 0.1438, 'grad_norm': 22.625, 'learning_rate': 0.0, 'epoch': 0.96}\n",
            "{'train_runtime': 32.9531, 'train_samples_per_second': 1.457, 'train_steps_per_second': 0.182, 'train_loss': 0.01293886328736941, 'epoch': 0.96}\n",
            "100% 6/6 [00:32<00:00,  5.49s/it]\n",
            "Forget step completed successfully.\n",
            "===== Step 2: Running evaluation =====\n",
            "Running evaluate_Harry.py...\n",
            "2025-05-21 11:04:53.823382: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1747825493.845178   87691 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1747825493.851831   87691 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "Loading checkpoint from /content/Unlearn_Harry/Harry/experiment/Harry/deepseek-r1-distill/grad_ascent+kl_E1_B2_G4_lr2e-5_W1\n",
            "Working on eval task eval_log_forget with split forget\n",
            "load data from  data/Harry/forget.json\n",
            "load data from  data/Harry/forget.json\n",
            "load data from  data/Harry/forget.json\n",
            "100% 13/13 [00:04<00:00,  2.71it/s]\n",
            "[2025-05-21 11:05:09,587][absl][INFO] - Using default tokenizer.\n",
            "fluency 3.5701116465251976\n",
            "Working on eval task eval_log_forget_rephrase with split forget\n",
            "load data from  data/Harry/forget.json\n",
            "load data from  data/Harry/forget.json\n",
            "load data from  data/Harry/forget.json\n",
            "100% 13/13 [00:04<00:00,  3.19it/s]\n",
            "[2025-05-21 11:05:13,714][absl][INFO] - Using default tokenizer.\n",
            "fluency 3.590679745272794\n",
            "Working on eval task eval_log_retain with split test_retain_harry\n",
            "load data from  data/test/test_retain_harry.json\n",
            "load data from  data/test/test_retain_harry.json\n",
            "load data from  data/test/test_retain_harry.json\n",
            "100% 38/38 [00:11<00:00,  3.17it/s]\n",
            "[2025-05-21 11:05:25,734][absl][INFO] - Using default tokenizer.\n",
            "fluency 3.499890371551867\n",
            "Working on eval task eval_log_retain_rephrase with split test_retain_harry\n",
            "load data from  data/test/test_retain_harry.json\n",
            "load data from  data/test/test_retain_harry.json\n",
            "load data from  data/test/test_retain_harry.json\n",
            "100% 38/38 [00:11<00:00,  3.18it/s]\n",
            "[2025-05-21 11:05:37,806][absl][INFO] - Using default tokenizer.\n",
            "fluency 3.504368302248082\n",
            "Working on eval task eval_real_author_wo_options with split real_authors_perturbed\n",
            "load data from  data/test/real_authors_perturbed.json\n",
            "load data from  data/test/real_authors_perturbed.json\n",
            "load data from  data/test/real_authors_perturbed.json\n",
            "  0% 0/25 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:497: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
            "  warnings.warn(\n",
            "100% 25/25 [00:15<00:00,  1.58it/s]\n",
            "[2025-05-21 11:05:53,703][absl][INFO] - Using default tokenizer.\n",
            "fluency 0.18144862502644238\n",
            "100it [00:57,  1.74it/s]\n",
            "Working on eval task eval_real_world_wo_options with split world_facts_perturbed\n",
            "load data from  data/test/world_facts_perturbed.json\n",
            "load data from  data/test/world_facts_perturbed.json\n",
            "load data from  data/test/world_facts_perturbed.json\n",
            "100% 30/30 [00:17<00:00,  1.68it/s]\n",
            "[2025-05-21 11:07:09,204][absl][INFO] - Using default tokenizer.\n",
            "fluency 0.29723583078147575\n",
            "117it [01:07,  1.74it/s]\n",
            "Evaluation completed successfully.\n",
            "===== Step 3: Aggregating statistics =====\n",
            "Running aggregate_eval_stat.py...\n",
            "[2025-05-21 11:08:19,230][numexpr.utils][INFO] - NumExpr defaulting to 12 threads.\n",
            "===== All steps completed successfully! =====\n",
            "Results saved to: /content/Unlearn_Harry/Harry/experiment/Harry/deepseek-r1-distill/grad_ascent+kl_E1_B2_G4_lr2e-5_W1/eval_results/\n"
          ]
        }
      ]
    }
  ]
}